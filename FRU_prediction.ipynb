{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(\"FRU_train.csv\")\n",
    "dataset\n",
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FRU</th>\n",
       "      <th>FRU_score</th>\n",
       "      <th>student</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67929500.19</td>\n",
       "      <td>28.82</td>\n",
       "      <td>7461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43590462.26</td>\n",
       "      <td>26.00</td>\n",
       "      <td>8474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41965535.59</td>\n",
       "      <td>23.00</td>\n",
       "      <td>7628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56425814.96</td>\n",
       "      <td>26.00</td>\n",
       "      <td>6757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26853427.52</td>\n",
       "      <td>18.00</td>\n",
       "      <td>11221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24839565.10</td>\n",
       "      <td>19.00</td>\n",
       "      <td>7243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29860030.77</td>\n",
       "      <td>20.00</td>\n",
       "      <td>6062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41940928.70</td>\n",
       "      <td>25.00</td>\n",
       "      <td>2095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19982059.15</td>\n",
       "      <td>18.00</td>\n",
       "      <td>5790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>34881917.57</td>\n",
       "      <td>23.00</td>\n",
       "      <td>1586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24061932.12</td>\n",
       "      <td>18.00</td>\n",
       "      <td>6551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19531198.72</td>\n",
       "      <td>16.00</td>\n",
       "      <td>7866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22319713.52</td>\n",
       "      <td>17.00</td>\n",
       "      <td>5577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16851677.36</td>\n",
       "      <td>14.00</td>\n",
       "      <td>6594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21448377.67</td>\n",
       "      <td>15.00</td>\n",
       "      <td>4959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17676314.99</td>\n",
       "      <td>15.00</td>\n",
       "      <td>3969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>28098141.44</td>\n",
       "      <td>21.00</td>\n",
       "      <td>1722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15227012.74</td>\n",
       "      <td>13.00</td>\n",
       "      <td>5019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>50872160.88</td>\n",
       "      <td>27.00</td>\n",
       "      <td>1276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>29063298.06</td>\n",
       "      <td>23.00</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>30652571.53</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>17284794.11</td>\n",
       "      <td>15.00</td>\n",
       "      <td>4523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25508875.16</td>\n",
       "      <td>21.00</td>\n",
       "      <td>1742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>43211655.46</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17119491.88</td>\n",
       "      <td>16.00</td>\n",
       "      <td>5106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16572468.21</td>\n",
       "      <td>16.00</td>\n",
       "      <td>4932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12274355.56</td>\n",
       "      <td>12.00</td>\n",
       "      <td>3591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13531279.15</td>\n",
       "      <td>12.00</td>\n",
       "      <td>4642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>14005105.38</td>\n",
       "      <td>12.00</td>\n",
       "      <td>5609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12606711.10</td>\n",
       "      <td>12.00</td>\n",
       "      <td>4059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>28110315.35</td>\n",
       "      <td>21.00</td>\n",
       "      <td>1097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>11647315.97</td>\n",
       "      <td>11.00</td>\n",
       "      <td>5722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>15626096.73</td>\n",
       "      <td>15.00</td>\n",
       "      <td>787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>11064492.28</td>\n",
       "      <td>11.00</td>\n",
       "      <td>4535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>12470316.14</td>\n",
       "      <td>14.00</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10391815.80</td>\n",
       "      <td>11.00</td>\n",
       "      <td>4421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>26180604.40</td>\n",
       "      <td>18.00</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>11384307.58</td>\n",
       "      <td>12.00</td>\n",
       "      <td>4181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>16682957.66</td>\n",
       "      <td>14.00</td>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>15461438.55</td>\n",
       "      <td>14.00</td>\n",
       "      <td>3343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>14065344.79</td>\n",
       "      <td>13.00</td>\n",
       "      <td>1719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>11636508.09</td>\n",
       "      <td>12.00</td>\n",
       "      <td>3655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>10240048.28</td>\n",
       "      <td>10.00</td>\n",
       "      <td>3387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>17745755.09</td>\n",
       "      <td>14.00</td>\n",
       "      <td>970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            FRU  FRU_score  student\n",
       "0   67929500.19      28.82     7461\n",
       "1   43590462.26      26.00     8474\n",
       "2   41965535.59      23.00     7628\n",
       "3   56425814.96      26.00     6757\n",
       "4   26853427.52      18.00    11221\n",
       "5   24839565.10      19.00     7243\n",
       "6   29860030.77      20.00     6062\n",
       "7   41940928.70      25.00     2095\n",
       "8   19982059.15      18.00     5790\n",
       "9   34881917.57      23.00     1586\n",
       "10  24061932.12      18.00     6551\n",
       "11  19531198.72      16.00     7866\n",
       "12  22319713.52      17.00     5577\n",
       "13  16851677.36      14.00     6594\n",
       "14  21448377.67      15.00     4959\n",
       "15  17676314.99      15.00     3969\n",
       "16  28098141.44      21.00     1722\n",
       "17  15227012.74      13.00     5019\n",
       "18  50872160.88      27.00     1276\n",
       "19  29063298.06      23.00     1617\n",
       "20  30652571.53      22.00     1684\n",
       "21  17284794.11      15.00     4523\n",
       "22  25508875.16      21.00     1742\n",
       "23  43211655.46      24.00     1084\n",
       "24  17119491.88      16.00     5106\n",
       "25  16572468.21      16.00     4932\n",
       "26  12274355.56      12.00     3591\n",
       "27  13531279.15      12.00     4642\n",
       "28  14005105.38      12.00     5609\n",
       "29  12606711.10      12.00     4059\n",
       "30  28110315.35      21.00     1097\n",
       "31  11647315.97      11.00     5722\n",
       "32  15626096.73      15.00      787\n",
       "33  11064492.28      11.00     4535\n",
       "34  12470316.14      14.00      498\n",
       "35  10391815.80      11.00     4421\n",
       "36  26180604.40      18.00      364\n",
       "37  11384307.58      12.00     4181\n",
       "38  16682957.66      14.00      668\n",
       "39  15461438.55      14.00     3343\n",
       "40  14065344.79      13.00     1719\n",
       "41  11636508.09      12.00     3655\n",
       "42  10240048.28      10.00     3387\n",
       "43  17745755.09      14.00      970"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset['student']  ## USe it to delete the column student\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FRU</th>\n",
       "      <th>FRU_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67929500.19</td>\n",
       "      <td>28.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43590462.26</td>\n",
       "      <td>26.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41965535.59</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56425814.96</td>\n",
       "      <td>26.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26853427.52</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24839565.10</td>\n",
       "      <td>19.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29860030.77</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41940928.70</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19982059.15</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>34881917.57</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24061932.12</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19531198.72</td>\n",
       "      <td>16.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22319713.52</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16851677.36</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21448377.67</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17676314.99</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>28098141.44</td>\n",
       "      <td>21.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15227012.74</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>50872160.88</td>\n",
       "      <td>27.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>29063298.06</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>30652571.53</td>\n",
       "      <td>22.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>17284794.11</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25508875.16</td>\n",
       "      <td>21.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>43211655.46</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17119491.88</td>\n",
       "      <td>16.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16572468.21</td>\n",
       "      <td>16.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12274355.56</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13531279.15</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>14005105.38</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12606711.10</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>28110315.35</td>\n",
       "      <td>21.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>11647315.97</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>15626096.73</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>11064492.28</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>12470316.14</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10391815.80</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>26180604.40</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>11384307.58</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>16682957.66</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>15461438.55</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>14065344.79</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>11636508.09</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>10240048.28</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>17745755.09</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            FRU  FRU_score\n",
       "0   67929500.19      28.82\n",
       "1   43590462.26      26.00\n",
       "2   41965535.59      23.00\n",
       "3   56425814.96      26.00\n",
       "4   26853427.52      18.00\n",
       "5   24839565.10      19.00\n",
       "6   29860030.77      20.00\n",
       "7   41940928.70      25.00\n",
       "8   19982059.15      18.00\n",
       "9   34881917.57      23.00\n",
       "10  24061932.12      18.00\n",
       "11  19531198.72      16.00\n",
       "12  22319713.52      17.00\n",
       "13  16851677.36      14.00\n",
       "14  21448377.67      15.00\n",
       "15  17676314.99      15.00\n",
       "16  28098141.44      21.00\n",
       "17  15227012.74      13.00\n",
       "18  50872160.88      27.00\n",
       "19  29063298.06      23.00\n",
       "20  30652571.53      22.00\n",
       "21  17284794.11      15.00\n",
       "22  25508875.16      21.00\n",
       "23  43211655.46      24.00\n",
       "24  17119491.88      16.00\n",
       "25  16572468.21      16.00\n",
       "26  12274355.56      12.00\n",
       "27  13531279.15      12.00\n",
       "28  14005105.38      12.00\n",
       "29  12606711.10      12.00\n",
       "30  28110315.35      21.00\n",
       "31  11647315.97      11.00\n",
       "32  15626096.73      15.00\n",
       "33  11064492.28      11.00\n",
       "34  12470316.14      14.00\n",
       "35  10391815.80      11.00\n",
       "36  26180604.40      18.00\n",
       "37  11384307.58      12.00\n",
       "38  16682957.66      14.00\n",
       "39  15461438.55      14.00\n",
       "40  14065344.79      13.00\n",
       "41  11636508.09      12.00\n",
       "42  10240048.28      10.00\n",
       "43  17745755.09      14.00"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=dataset.iloc[:,:-1].values\n",
    "X=dataset.iloc[:,1].values\n",
    "#y=dataset.iloc[:,1].values\n",
    "#X=dataset.iloc[:,:-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28.82, 26.  , 23.  , 26.  , 18.  , 19.  , 20.  , 25.  , 18.  ,\n",
       "       23.  , 18.  , 16.  , 17.  , 14.  , 15.  , 15.  , 21.  , 13.  ,\n",
       "       27.  , 23.  , 22.  , 15.  , 21.  , 24.  , 16.  , 16.  , 12.  ,\n",
       "       12.  , 12.  , 12.  , 21.  , 11.  , 15.  , 11.  , 14.  , 11.  ,\n",
       "       18.  , 12.  , 14.  , 14.  , 13.  , 12.  , 10.  , 14.  ])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#X= np.reshape(X, (1, -1))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[67929500.19],\n",
       "       [43590462.26],\n",
       "       [41965535.59],\n",
       "       [56425814.96],\n",
       "       [26853427.52],\n",
       "       [24839565.1 ],\n",
       "       [29860030.77],\n",
       "       [41940928.7 ],\n",
       "       [19982059.15],\n",
       "       [34881917.57],\n",
       "       [24061932.12],\n",
       "       [19531198.72],\n",
       "       [22319713.52],\n",
       "       [16851677.36],\n",
       "       [21448377.67],\n",
       "       [17676314.99],\n",
       "       [28098141.44],\n",
       "       [15227012.74],\n",
       "       [50872160.88],\n",
       "       [29063298.06],\n",
       "       [30652571.53],\n",
       "       [17284794.11],\n",
       "       [25508875.16],\n",
       "       [43211655.46],\n",
       "       [17119491.88],\n",
       "       [16572468.21],\n",
       "       [12274355.56],\n",
       "       [13531279.15],\n",
       "       [14005105.38],\n",
       "       [12606711.1 ],\n",
       "       [28110315.35],\n",
       "       [11647315.97],\n",
       "       [15626096.73],\n",
       "       [11064492.28],\n",
       "       [12470316.14],\n",
       "       [10391815.8 ],\n",
       "       [26180604.4 ],\n",
       "       [11384307.58],\n",
       "       [16682957.66],\n",
       "       [15461438.55],\n",
       "       [14065344.79],\n",
       "       [11636508.09],\n",
       "       [10240048.28],\n",
       "       [17745755.09]])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaler = preprocessing.StandardScaler().fit(X)\n",
    "#X = scaler.transform(X)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[67929500.19],\n",
       "       [43590462.26],\n",
       "       [41965535.59],\n",
       "       [56425814.96],\n",
       "       [26853427.52],\n",
       "       [24839565.1 ],\n",
       "       [29860030.77],\n",
       "       [41940928.7 ],\n",
       "       [19982059.15],\n",
       "       [34881917.57],\n",
       "       [24061932.12],\n",
       "       [19531198.72],\n",
       "       [22319713.52],\n",
       "       [16851677.36],\n",
       "       [21448377.67],\n",
       "       [17676314.99],\n",
       "       [28098141.44],\n",
       "       [15227012.74],\n",
       "       [50872160.88],\n",
       "       [29063298.06],\n",
       "       [30652571.53],\n",
       "       [17284794.11],\n",
       "       [25508875.16],\n",
       "       [43211655.46],\n",
       "       [17119491.88],\n",
       "       [16572468.21],\n",
       "       [12274355.56],\n",
       "       [13531279.15],\n",
       "       [14005105.38],\n",
       "       [12606711.1 ],\n",
       "       [28110315.35],\n",
       "       [11647315.97],\n",
       "       [15626096.73],\n",
       "       [11064492.28],\n",
       "       [12470316.14],\n",
       "       [10391815.8 ],\n",
       "       [26180604.4 ],\n",
       "       [11384307.58],\n",
       "       [16682957.66],\n",
       "       [15461438.55],\n",
       "       [14065344.79],\n",
       "       [11636508.09],\n",
       "       [10240048.28],\n",
       "       [17745755.09]])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18638476.905"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Med=np.median(y)\n",
    "Med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y/Med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n",
    "len(y)\n",
    "#len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(y,X,test_size=0.1,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly=PolynomialFeatures(degree =1)\n",
    "X_poly=poly.fit_transform(X_train)\n",
    "poly.fit(X_poly,y_train)\n",
    "lin2=LinearRegression()\n",
    "lin2.fit(X_poly,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train_predicted=lin2.predict(X_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-275-eaad0ac3e729>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-275-eaad0ac3e729>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    cost = np.sum((y_true-y_predicted)**2) / len(y_true)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#def mean_squared_error(y_true, y_predicted):\n",
    "     \n",
    "    # Calculating the loss or cost\n",
    "    cost = np.sum((y_true-y_predicted)**2) / len(y_true)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def gradient_descent(x, y, iterations = 1000, learning_rate = 0.0001,\n",
    "                     stopping_threshold = 1e-6):\n",
    "     \n",
    "    # Initializing weight, bias, learning rate and iterations\n",
    "    current_weight = 0.1\n",
    "    current_bias = 0.01\n",
    "    iterations = iterations\n",
    "    learning_rate = learning_rate\n",
    "    n = float(len(x))\n",
    "     \n",
    "    costs = []\n",
    "    weights = []\n",
    "    previous_cost = None\n",
    "     \n",
    "    # Estimation of optimal parameters\n",
    "    for i in range(iterations):\n",
    "         \n",
    "        # Making predictions\n",
    "        y_predicted = (current_weight * x) + current_bias\n",
    "         \n",
    "        # Calculationg the current cost\n",
    "        current_cost = mean_squared_error(y, y_predicted)\n",
    " \n",
    "        # If the change in cost is less than or equal to\n",
    "        # stopping_threshold we stop the gradient descent\n",
    "        if previous_cost and abs(previous_cost-current_cost)<=stopping_threshold:\n",
    "            break\n",
    "         \n",
    "        previous_cost = current_cost\n",
    " \n",
    "        costs.append(current_cost)\n",
    "        weights.append(current_weight)\n",
    "         \n",
    "        # Calculating the gradients\n",
    "        weight_derivative = -(2/n) * sum(x * (y-y_predicted))\n",
    "        bias_derivative = -(2/n) * sum(y-y_predicted)\n",
    "         \n",
    "        # Updating weights and bias\n",
    "        current_weight = current_weight - (learning_rate * weight_derivative)\n",
    "        current_bias = current_bias - (learning_rate * bias_derivative)\n",
    "                 \n",
    "        # Printing the parameters for each 1000th iteration\n",
    "        print(f\"Iteration {i+1}: Cost {current_cost}, Weight \\\n",
    "        {current_weight}, Bias {current_bias}\")\n",
    "     \n",
    "     \n",
    "    # Visualizing the weights and cost at for all iterations\n",
    "    plt.figure(figsize = (8,6))\n",
    "    plt.plot(weights, costs)\n",
    "    plt.scatter(weights, costs, marker='o', color='red')\n",
    "    plt.title(\"Cost vs Weights\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.xlabel(\"Weight\")\n",
    "    plt.show()\n",
    "     \n",
    "    return current_weight, current_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Cost 326872728331778.06, Weight         [-1.48578781e+10 -1.48578915e+10 -1.48579058e+10 -1.48578915e+10\n",
      " -1.48579297e+10 -1.48579249e+10 -1.48579201e+10 -1.48578963e+10\n",
      " -1.48579297e+10 -1.48579058e+10 -1.48579297e+10 -1.48579392e+10\n",
      " -1.48579344e+10 -1.48579487e+10 -1.48579440e+10 -1.48579440e+10\n",
      " -1.48579153e+10 -1.48579535e+10 -1.48578867e+10 -1.48579058e+10\n",
      " -1.48579106e+10 -1.48579440e+10 -1.48579153e+10 -1.48579010e+10\n",
      " -1.48579392e+10 -1.48579392e+10 -1.48579583e+10 -1.48579583e+10\n",
      " -1.48579583e+10 -1.48579583e+10 -1.48579153e+10 -1.48579630e+10\n",
      " -1.48579440e+10 -1.48579630e+10 -1.48579487e+10 -1.48579630e+10\n",
      " -1.48579297e+10 -1.48579583e+10 -1.48579487e+10 -1.48579487e+10\n",
      " -1.48579535e+10 -1.48579583e+10 -1.48579678e+10 -1.48579487e+10], Bias [-476.75409871 -476.75466271 -476.75526271 -476.75466271 -476.75626271\n",
      " -476.75606271 -476.75586271 -476.75486271 -476.75626271 -476.75526271\n",
      " -476.75626271 -476.75666271 -476.75646271 -476.75706271 -476.75686271\n",
      " -476.75686271 -476.75566271 -476.75726271 -476.75446271 -476.75526271\n",
      " -476.75546271 -476.75686271 -476.75566271 -476.75506271 -476.75666271\n",
      " -476.75666271 -476.75746271 -476.75746271 -476.75746271 -476.75746271\n",
      " -476.75566271 -476.75766271 -476.75686271 -476.75766271 -476.75706271\n",
      " -476.75766271 -476.75626271 -476.75746271 -476.75706271 -476.75706271\n",
      " -476.75726271 -476.75746271 -476.75786271 -476.75706271]\n",
      "Iteration 2: Cost 7.216062690830378e+36, Weight         [2.20758581e+21 2.20758781e+21 2.20758994e+21 2.20758781e+21\n",
      " 2.20759348e+21 2.20759277e+21 2.20759206e+21 2.20758852e+21\n",
      " 2.20759348e+21 2.20758994e+21 2.20759348e+21 2.20759490e+21\n",
      " 2.20759419e+21 2.20759631e+21 2.20759560e+21 2.20759560e+21\n",
      " 2.20759135e+21 2.20759702e+21 2.20758710e+21 2.20758994e+21\n",
      " 2.20759065e+21 2.20759560e+21 2.20759135e+21 2.20758923e+21\n",
      " 2.20759490e+21 2.20759490e+21 2.20759773e+21 2.20759773e+21\n",
      " 2.20759773e+21 2.20759773e+21 2.20759135e+21 2.20759844e+21\n",
      " 2.20759560e+21 2.20759844e+21 2.20759631e+21 2.20759844e+21\n",
      " 2.20759348e+21 2.20759773e+21 2.20759631e+21 2.20759631e+21\n",
      " 2.20759702e+21 2.20759773e+21 2.20759915e+21 2.20759631e+21], Bias [7.08378846e+13 7.08379487e+13 7.08380168e+13 7.08379487e+13\n",
      " 7.08381305e+13 7.08381078e+13 7.08380850e+13 7.08379714e+13\n",
      " 7.08381305e+13 7.08380168e+13 7.08381305e+13 7.08381760e+13\n",
      " 7.08381532e+13 7.08382214e+13 7.08381987e+13 7.08381987e+13\n",
      " 7.08380623e+13 7.08382442e+13 7.08379259e+13 7.08380168e+13\n",
      " 7.08380396e+13 7.08381987e+13 7.08380623e+13 7.08379941e+13\n",
      " 7.08381760e+13 7.08381760e+13 7.08382669e+13 7.08382669e+13\n",
      " 7.08382669e+13 7.08382669e+13 7.08380623e+13 7.08382896e+13\n",
      " 7.08381987e+13 7.08382896e+13 7.08382214e+13 7.08382896e+13\n",
      " 7.08381305e+13 7.08382669e+13 7.08382214e+13 7.08382214e+13\n",
      " 7.08382442e+13 7.08382669e+13 7.08383123e+13 7.08382214e+13]\n",
      "Iteration 3: Cost 1.593022489954287e+59, Weight         [-3.28003441e+32 -3.28003738e+32 -3.28004053e+32 -3.28003738e+32\n",
      " -3.28004580e+32 -3.28004474e+32 -3.28004369e+32 -3.28003843e+32\n",
      " -3.28004580e+32 -3.28004053e+32 -3.28004580e+32 -3.28004790e+32\n",
      " -3.28004685e+32 -3.28005001e+32 -3.28004895e+32 -3.28004895e+32\n",
      " -3.28004264e+32 -3.28005106e+32 -3.28003632e+32 -3.28004053e+32\n",
      " -3.28004159e+32 -3.28004895e+32 -3.28004264e+32 -3.28003948e+32\n",
      " -3.28004790e+32 -3.28004790e+32 -3.28005211e+32 -3.28005211e+32\n",
      " -3.28005211e+32 -3.28005211e+32 -3.28004264e+32 -3.28005316e+32\n",
      " -3.28004895e+32 -3.28005316e+32 -3.28005001e+32 -3.28005316e+32\n",
      " -3.28004580e+32 -3.28005211e+32 -3.28005001e+32 -3.28005001e+32\n",
      " -3.28005106e+32 -3.28005211e+32 -3.28005422e+32 -3.28005001e+32], Bias [-1.05251038e+25 -1.05251133e+25 -1.05251235e+25 -1.05251133e+25\n",
      " -1.05251404e+25 -1.05251370e+25 -1.05251336e+25 -1.05251167e+25\n",
      " -1.05251404e+25 -1.05251235e+25 -1.05251404e+25 -1.05251471e+25\n",
      " -1.05251437e+25 -1.05251539e+25 -1.05251505e+25 -1.05251505e+25\n",
      " -1.05251302e+25 -1.05251572e+25 -1.05251100e+25 -1.05251235e+25\n",
      " -1.05251268e+25 -1.05251505e+25 -1.05251302e+25 -1.05251201e+25\n",
      " -1.05251471e+25 -1.05251471e+25 -1.05251606e+25 -1.05251606e+25\n",
      " -1.05251606e+25 -1.05251606e+25 -1.05251302e+25 -1.05251640e+25\n",
      " -1.05251505e+25 -1.05251640e+25 -1.05251539e+25 -1.05251640e+25\n",
      " -1.05251404e+25 -1.05251606e+25 -1.05251539e+25 -1.05251539e+25\n",
      " -1.05251572e+25 -1.05251606e+25 -1.05251674e+25 -1.05251539e+25]\n",
      "Iteration 4: Cost 3.516766361695967e+81, Weight         [4.87348018e+43 4.87348459e+43 4.87348928e+43 4.87348459e+43\n",
      " 4.87349710e+43 4.87349554e+43 4.87349397e+43 4.87348615e+43\n",
      " 4.87349710e+43 4.87348928e+43 4.87349710e+43 4.87350023e+43\n",
      " 4.87349866e+43 4.87350336e+43 4.87350179e+43 4.87350179e+43\n",
      " 4.87349241e+43 4.87350492e+43 4.87348303e+43 4.87348928e+43\n",
      " 4.87349085e+43 4.87350179e+43 4.87349241e+43 4.87348772e+43\n",
      " 4.87350023e+43 4.87350023e+43 4.87350648e+43 4.87350648e+43\n",
      " 4.87350648e+43 4.87350648e+43 4.87349241e+43 4.87350805e+43\n",
      " 4.87350179e+43 4.87350805e+43 4.87350336e+43 4.87350805e+43\n",
      " 4.87349710e+43 4.87350648e+43 4.87350336e+43 4.87350336e+43\n",
      " 4.87350492e+43 4.87350648e+43 4.87350961e+43 4.87350336e+43], Bias [1.56382155e+36 1.56382296e+36 1.56382447e+36 1.56382296e+36\n",
      " 1.56382698e+36 1.56382648e+36 1.56382597e+36 1.56382346e+36\n",
      " 1.56382698e+36 1.56382447e+36 1.56382698e+36 1.56382798e+36\n",
      " 1.56382748e+36 1.56382898e+36 1.56382848e+36 1.56382848e+36\n",
      " 1.56382547e+36 1.56382949e+36 1.56382246e+36 1.56382447e+36\n",
      " 1.56382497e+36 1.56382848e+36 1.56382547e+36 1.56382397e+36\n",
      " 1.56382798e+36 1.56382798e+36 1.56382999e+36 1.56382999e+36\n",
      " 1.56382999e+36 1.56382999e+36 1.56382547e+36 1.56383049e+36\n",
      " 1.56382848e+36 1.56383049e+36 1.56382898e+36 1.56383049e+36\n",
      " 1.56382698e+36 1.56382999e+36 1.56382898e+36 1.56382898e+36\n",
      " 1.56382949e+36 1.56382999e+36 1.56383099e+36 1.56382898e+36]\n",
      "Iteration 5: Cost 7.763635303799883e+103, Weight         [-7.24102437e+54 -7.24103092e+54 -7.24103789e+54 -7.24103092e+54\n",
      " -7.24104951e+54 -7.24104718e+54 -7.24104486e+54 -7.24103324e+54\n",
      " -7.24104951e+54 -7.24103789e+54 -7.24104951e+54 -7.24105416e+54\n",
      " -7.24105183e+54 -7.24105880e+54 -7.24105648e+54 -7.24105648e+54\n",
      " -7.24104254e+54 -7.24106113e+54 -7.24102860e+54 -7.24103789e+54\n",
      " -7.24104021e+54 -7.24105648e+54 -7.24104254e+54 -7.24103557e+54\n",
      " -7.24105416e+54 -7.24105416e+54 -7.24106345e+54 -7.24106345e+54\n",
      " -7.24106345e+54 -7.24106345e+54 -7.24104254e+54 -7.24106577e+54\n",
      " -7.24105648e+54 -7.24106577e+54 -7.24105880e+54 -7.24106577e+54\n",
      " -7.24104951e+54 -7.24106345e+54 -7.24105880e+54 -7.24105880e+54\n",
      " -7.24106113e+54 -7.24106345e+54 -7.24106810e+54 -7.24105880e+54], Bias [-2.32352847e+47 -2.32353057e+47 -2.32353281e+47 -2.32353057e+47\n",
      " -2.32353653e+47 -2.32353579e+47 -2.32353504e+47 -2.32353131e+47\n",
      " -2.32353653e+47 -2.32353281e+47 -2.32353653e+47 -2.32353803e+47\n",
      " -2.32353728e+47 -2.32353952e+47 -2.32353877e+47 -2.32353877e+47\n",
      " -2.32353430e+47 -2.32354026e+47 -2.32352982e+47 -2.32353281e+47\n",
      " -2.32353355e+47 -2.32353877e+47 -2.32353430e+47 -2.32353206e+47\n",
      " -2.32353803e+47 -2.32353803e+47 -2.32354101e+47 -2.32354101e+47\n",
      " -2.32354101e+47 -2.32354101e+47 -2.32353430e+47 -2.32354175e+47\n",
      " -2.32353877e+47 -2.32354175e+47 -2.32353952e+47 -2.32354175e+47\n",
      " -2.32353653e+47 -2.32354101e+47 -2.32353952e+47 -2.32353952e+47\n",
      " -2.32354026e+47 -2.32354101e+47 -2.32354250e+47 -2.32353952e+47]\n",
      "Iteration 6: Cost 1.7139049607304763e+126, Weight         [1.07587252e+66 1.07587349e+66 1.07587453e+66 1.07587349e+66\n",
      " 1.07587625e+66 1.07587591e+66 1.07587556e+66 1.07587384e+66\n",
      " 1.07587625e+66 1.07587453e+66 1.07587625e+66 1.07587694e+66\n",
      " 1.07587660e+66 1.07587763e+66 1.07587729e+66 1.07587729e+66\n",
      " 1.07587522e+66 1.07587798e+66 1.07587315e+66 1.07587453e+66\n",
      " 1.07587487e+66 1.07587729e+66 1.07587522e+66 1.07587418e+66\n",
      " 1.07587694e+66 1.07587694e+66 1.07587832e+66 1.07587832e+66\n",
      " 1.07587832e+66 1.07587832e+66 1.07587522e+66 1.07587867e+66\n",
      " 1.07587729e+66 1.07587867e+66 1.07587763e+66 1.07587867e+66\n",
      " 1.07587625e+66 1.07587832e+66 1.07587763e+66 1.07587763e+66\n",
      " 1.07587798e+66 1.07587832e+66 1.07587901e+66 1.07587763e+66], Bias [3.45230218e+58 3.45230530e+58 3.45230863e+58 3.45230530e+58\n",
      " 3.45231417e+58 3.45231306e+58 3.45231195e+58 3.45230641e+58\n",
      " 3.45231417e+58 3.45230863e+58 3.45231417e+58 3.45231638e+58\n",
      " 3.45231527e+58 3.45231860e+58 3.45231749e+58 3.45231749e+58\n",
      " 3.45231084e+58 3.45231970e+58 3.45230420e+58 3.45230863e+58\n",
      " 3.45230973e+58 3.45231749e+58 3.45231084e+58 3.45230752e+58\n",
      " 3.45231638e+58 3.45231638e+58 3.45232081e+58 3.45232081e+58\n",
      " 3.45232081e+58 3.45232081e+58 3.45231084e+58 3.45232192e+58\n",
      " 3.45231749e+58 3.45232192e+58 3.45231860e+58 3.45232192e+58\n",
      " 3.45231417e+58 3.45232081e+58 3.45231860e+58 3.45231860e+58\n",
      " 3.45231970e+58 3.45232081e+58 3.45232303e+58 3.45231860e+58]\n",
      "Iteration 7: Cost 3.783627256394697e+148, Weight         [-1.59853305e+77 -1.59853449e+77 -1.59853603e+77 -1.59853449e+77\n",
      " -1.59853860e+77 -1.59853808e+77 -1.59853757e+77 -1.59853500e+77\n",
      " -1.59853860e+77 -1.59853603e+77 -1.59853860e+77 -1.59853962e+77\n",
      " -1.59853911e+77 -1.59854065e+77 -1.59854013e+77 -1.59854013e+77\n",
      " -1.59853706e+77 -1.59854116e+77 -1.59853398e+77 -1.59853603e+77\n",
      " -1.59853654e+77 -1.59854013e+77 -1.59853706e+77 -1.59853552e+77\n",
      " -1.59853962e+77 -1.59853962e+77 -1.59854167e+77 -1.59854167e+77\n",
      " -1.59854167e+77 -1.59854167e+77 -1.59853706e+77 -1.59854219e+77\n",
      " -1.59854013e+77 -1.59854219e+77 -1.59854065e+77 -1.59854219e+77\n",
      " -1.59853860e+77 -1.59854167e+77 -1.59854065e+77 -1.59854065e+77\n",
      " -1.59854116e+77 -1.59854167e+77 -1.59854270e+77 -1.59854065e+77], Bias [-5.12943590e+69 -5.12944054e+69 -5.12944548e+69 -5.12944054e+69\n",
      " -5.12945371e+69 -5.12945207e+69 -5.12945042e+69 -5.12944219e+69\n",
      " -5.12945371e+69 -5.12944548e+69 -5.12945371e+69 -5.12945700e+69\n",
      " -5.12945536e+69 -5.12946029e+69 -5.12945865e+69 -5.12945865e+69\n",
      " -5.12944877e+69 -5.12946194e+69 -5.12943890e+69 -5.12944548e+69\n",
      " -5.12944713e+69 -5.12945865e+69 -5.12944877e+69 -5.12944384e+69\n",
      " -5.12945700e+69 -5.12945700e+69 -5.12946359e+69 -5.12946359e+69\n",
      " -5.12946359e+69 -5.12946359e+69 -5.12944877e+69 -5.12946523e+69\n",
      " -5.12945865e+69 -5.12946523e+69 -5.12946029e+69 -5.12946523e+69\n",
      " -5.12945371e+69 -5.12946359e+69 -5.12946029e+69 -5.12946029e+69\n",
      " -5.12946194e+69 -5.12946359e+69 -5.12946688e+69 -5.12946029e+69]\n",
      "Iteration 8: Cost 8.352759075527366e+170, Weight         [2.37510286e+88 2.37510501e+88 2.37510730e+88 2.37510501e+88\n",
      " 2.37511111e+88 2.37511035e+88 2.37510959e+88 2.37510578e+88\n",
      " 2.37511111e+88 2.37510730e+88 2.37511111e+88 2.37511263e+88\n",
      " 2.37511187e+88 2.37511416e+88 2.37511340e+88 2.37511340e+88\n",
      " 2.37510882e+88 2.37511492e+88 2.37510425e+88 2.37510730e+88\n",
      " 2.37510806e+88 2.37511340e+88 2.37510882e+88 2.37510654e+88\n",
      " 2.37511263e+88 2.37511263e+88 2.37511568e+88 2.37511568e+88\n",
      " 2.37511568e+88 2.37511568e+88 2.37510882e+88 2.37511645e+88\n",
      " 2.37511340e+88 2.37511645e+88 2.37511416e+88 2.37511645e+88\n",
      " 2.37511111e+88 2.37511568e+88 2.37511416e+88 2.37511416e+88\n",
      " 2.37511492e+88 2.37511568e+88 2.37511721e+88 2.37511416e+88], Bias [7.62132377e+80 7.62133067e+80 7.62133801e+80 7.62133067e+80\n",
      " 7.62135023e+80 7.62134779e+80 7.62134534e+80 7.62133312e+80\n",
      " 7.62135023e+80 7.62133801e+80 7.62135023e+80 7.62135513e+80\n",
      " 7.62135268e+80 7.62136002e+80 7.62135757e+80 7.62135757e+80\n",
      " 7.62134290e+80 7.62136246e+80 7.62132822e+80 7.62133801e+80\n",
      " 7.62134045e+80 7.62135757e+80 7.62134290e+80 7.62133556e+80\n",
      " 7.62135513e+80 7.62135513e+80 7.62136491e+80 7.62136491e+80\n",
      " 7.62136491e+80 7.62136491e+80 7.62134290e+80 7.62136735e+80\n",
      " 7.62135757e+80 7.62136735e+80 7.62136002e+80 7.62136735e+80\n",
      " 7.62135023e+80 7.62136491e+80 7.62136002e+80 7.62136002e+80\n",
      " 7.62136246e+80 7.62136491e+80 7.62136980e+80 7.62136002e+80]\n",
      "Iteration 9: Cost 1.843960291170043e+193, Weight         [-3.52893150e+99 -3.52893469e+99 -3.52893809e+99 -3.52893469e+99\n",
      " -3.52894375e+99 -3.52894262e+99 -3.52894149e+99 -3.52893582e+99\n",
      " -3.52894375e+99 -3.52893809e+99 -3.52894375e+99 -3.52894602e+99\n",
      " -3.52894488e+99 -3.52894828e+99 -3.52894715e+99 -3.52894715e+99\n",
      " -3.52894035e+99 -3.52894941e+99 -3.52893356e+99 -3.52893809e+99\n",
      " -3.52893922e+99 -3.52894715e+99 -3.52894035e+99 -3.52893696e+99\n",
      " -3.52894602e+99 -3.52894602e+99 -3.52895054e+99 -3.52895054e+99\n",
      " -3.52895054e+99 -3.52895054e+99 -3.52894035e+99 -3.52895168e+99\n",
      " -3.52894715e+99 -3.52895168e+99 -3.52894828e+99 -3.52895168e+99\n",
      " -3.52894375e+99 -3.52895054e+99 -3.52894828e+99 -3.52894828e+99\n",
      " -3.52894941e+99 -3.52895054e+99 -3.52895281e+99 -3.52894828e+99], Bias [-1.13237746e+92 -1.13237849e+92 -1.13237958e+92 -1.13237849e+92\n",
      " -1.13238139e+92 -1.13238103e+92 -1.13238067e+92 -1.13237885e+92\n",
      " -1.13238139e+92 -1.13237958e+92 -1.13238139e+92 -1.13238212e+92\n",
      " -1.13238176e+92 -1.13238285e+92 -1.13238248e+92 -1.13238248e+92\n",
      " -1.13238030e+92 -1.13238321e+92 -1.13237812e+92 -1.13237958e+92\n",
      " -1.13237994e+92 -1.13238248e+92 -1.13238030e+92 -1.13237921e+92\n",
      " -1.13238212e+92 -1.13238212e+92 -1.13238357e+92 -1.13238357e+92\n",
      " -1.13238357e+92 -1.13238357e+92 -1.13238030e+92 -1.13238394e+92\n",
      " -1.13238248e+92 -1.13238394e+92 -1.13238285e+92 -1.13238394e+92\n",
      " -1.13238139e+92 -1.13238357e+92 -1.13238285e+92 -1.13238285e+92\n",
      " -1.13238321e+92 -1.13238357e+92 -1.13238430e+92 -1.13238285e+92]\n",
      "Iteration 10: Cost 4.07073821316609e+215, Weight         [5.24329186e+110 5.24329661e+110 5.24330165e+110 5.24329661e+110\n",
      " 5.24331007e+110 5.24330838e+110 5.24330670e+110 5.24329829e+110\n",
      " 5.24331007e+110 5.24330165e+110 5.24331007e+110 5.24331343e+110\n",
      " 5.24331175e+110 5.24331680e+110 5.24331511e+110 5.24331511e+110\n",
      " 5.24330502e+110 5.24331848e+110 5.24329492e+110 5.24330165e+110\n",
      " 5.24330334e+110 5.24331511e+110 5.24330502e+110 5.24329997e+110\n",
      " 5.24331343e+110 5.24331343e+110 5.24332016e+110 5.24332016e+110\n",
      " 5.24332016e+110 5.24332016e+110 5.24330502e+110 5.24332184e+110\n",
      " 5.24331511e+110 5.24332184e+110 5.24331680e+110 5.24332184e+110\n",
      " 5.24331007e+110 5.24332016e+110 5.24331680e+110 5.24331680e+110\n",
      " 5.24331848e+110 5.24332016e+110 5.24332353e+110 5.24331680e+110], Bias [1.68248818e+103 1.68248970e+103 1.68249132e+103 1.68248970e+103\n",
      " 1.68249402e+103 1.68249348e+103 1.68249294e+103 1.68249024e+103\n",
      " 1.68249402e+103 1.68249132e+103 1.68249402e+103 1.68249510e+103\n",
      " 1.68249456e+103 1.68249618e+103 1.68249564e+103 1.68249564e+103\n",
      " 1.68249240e+103 1.68249672e+103 1.68248916e+103 1.68249132e+103\n",
      " 1.68249186e+103 1.68249564e+103 1.68249240e+103 1.68249078e+103\n",
      " 1.68249510e+103 1.68249510e+103 1.68249726e+103 1.68249726e+103\n",
      " 1.68249726e+103 1.68249726e+103 1.68249240e+103 1.68249780e+103\n",
      " 1.68249564e+103 1.68249780e+103 1.68249618e+103 1.68249780e+103\n",
      " 1.68249402e+103 1.68249726e+103 1.68249618e+103 1.68249618e+103\n",
      " 1.68249672e+103 1.68249726e+103 1.68249834e+103 1.68249618e+103]\n",
      "Iteration 11: Cost 8.986587010296174e+237, Weight         [-7.79049113e+121 -7.79049818e+121 -7.79050568e+121 -7.79049818e+121\n",
      " -7.79051818e+121 -7.79051568e+121 -7.79051318e+121 -7.79050068e+121\n",
      " -7.79051818e+121 -7.79050568e+121 -7.79051818e+121 -7.79052318e+121\n",
      " -7.79052068e+121 -7.79052818e+121 -7.79052568e+121 -7.79052568e+121\n",
      " -7.79051068e+121 -7.79053068e+121 -7.79049568e+121 -7.79050568e+121\n",
      " -7.79050818e+121 -7.79052568e+121 -7.79051068e+121 -7.79050318e+121\n",
      " -7.79052318e+121 -7.79052318e+121 -7.79053318e+121 -7.79053318e+121\n",
      " -7.79053318e+121 -7.79053318e+121 -7.79051068e+121 -7.79053568e+121\n",
      " -7.79052568e+121 -7.79053568e+121 -7.79052818e+121 -7.79053568e+121\n",
      " -7.79051818e+121 -7.79053318e+121 -7.79052818e+121 -7.79052818e+121\n",
      " -7.79053068e+121 -7.79053318e+121 -7.79053818e+121 -7.79052818e+121], Bias [-2.49984353e+114 -2.49984579e+114 -2.49984820e+114 -2.49984579e+114\n",
      " -2.49985221e+114 -2.49985141e+114 -2.49985061e+114 -2.49984659e+114\n",
      " -2.49985221e+114 -2.49984820e+114 -2.49985221e+114 -2.49985381e+114\n",
      " -2.49985301e+114 -2.49985542e+114 -2.49985462e+114 -2.49985462e+114\n",
      " -2.49984980e+114 -2.49985622e+114 -2.49984499e+114 -2.49984820e+114\n",
      " -2.49984900e+114 -2.49985462e+114 -2.49984980e+114 -2.49984740e+114\n",
      " -2.49985381e+114 -2.49985381e+114 -2.49985702e+114 -2.49985702e+114\n",
      " -2.49985702e+114 -2.49985702e+114 -2.49984980e+114 -2.49985783e+114\n",
      " -2.49985462e+114 -2.49985783e+114 -2.49985542e+114 -2.49985783e+114\n",
      " -2.49985221e+114 -2.49985702e+114 -2.49985542e+114 -2.49985542e+114\n",
      " -2.49985622e+114 -2.49985702e+114 -2.49985863e+114 -2.49985542e+114]\n",
      "Iteration 12: Cost 1.983884540460594e+260, Weight         [1.15751237e+133 1.15751342e+133 1.15751454e+133 1.15751342e+133\n",
      " 1.15751639e+133 1.15751602e+133 1.15751565e+133 1.15751379e+133\n",
      " 1.15751639e+133 1.15751454e+133 1.15751639e+133 1.15751714e+133\n",
      " 1.15751676e+133 1.15751788e+133 1.15751751e+133 1.15751751e+133\n",
      " 1.15751528e+133 1.15751825e+133 1.15751305e+133 1.15751454e+133\n",
      " 1.15751491e+133 1.15751751e+133 1.15751528e+133 1.15751416e+133\n",
      " 1.15751714e+133 1.15751714e+133 1.15751862e+133 1.15751862e+133\n",
      " 1.15751862e+133 1.15751862e+133 1.15751528e+133 1.15751899e+133\n",
      " 1.15751751e+133 1.15751899e+133 1.15751788e+133 1.15751899e+133\n",
      " 1.15751639e+133 1.15751862e+133 1.15751788e+133 1.15751788e+133\n",
      " 1.15751825e+133 1.15751862e+133 1.15751936e+133 1.15751788e+133], Bias [3.71427137e+125 3.71427473e+125 3.71427831e+125 3.71427473e+125\n",
      " 3.71428427e+125 3.71428308e+125 3.71428189e+125 3.71427593e+125\n",
      " 3.71428427e+125 3.71427831e+125 3.71428427e+125 3.71428665e+125\n",
      " 3.71428546e+125 3.71428904e+125 3.71428784e+125 3.71428784e+125\n",
      " 3.71428069e+125 3.71429023e+125 3.71427354e+125 3.71427831e+125\n",
      " 3.71427950e+125 3.71428784e+125 3.71428069e+125 3.71427712e+125\n",
      " 3.71428665e+125 3.71428665e+125 3.71429142e+125 3.71429142e+125\n",
      " 3.71429142e+125 3.71429142e+125 3.71428069e+125 3.71429261e+125\n",
      " 3.71428784e+125 3.71429261e+125 3.71428904e+125 3.71429261e+125\n",
      " 3.71428427e+125 3.71429142e+125 3.71428904e+125 3.71428904e+125\n",
      " 3.71429023e+125 3.71429142e+125 3.71429380e+125 3.71428904e+125]\n",
      "Iteration 13: Cost 4.3796358566040613e+282, Weight         [-1.71983367e+144 -1.71983523e+144 -1.71983688e+144 -1.71983523e+144\n",
      " -1.71983964e+144 -1.71983909e+144 -1.71983854e+144 -1.71983578e+144\n",
      " -1.71983964e+144 -1.71983688e+144 -1.71983964e+144 -1.71984075e+144\n",
      " -1.71984019e+144 -1.71984185e+144 -1.71984130e+144 -1.71984130e+144\n",
      " -1.71983799e+144 -1.71984240e+144 -1.71983467e+144 -1.71983688e+144\n",
      " -1.71983743e+144 -1.71984130e+144 -1.71983799e+144 -1.71983633e+144\n",
      " -1.71984075e+144 -1.71984075e+144 -1.71984295e+144 -1.71984295e+144\n",
      " -1.71984295e+144 -1.71984295e+144 -1.71983799e+144 -1.71984350e+144\n",
      " -1.71984130e+144 -1.71984350e+144 -1.71984185e+144 -1.71984350e+144\n",
      " -1.71983964e+144 -1.71984295e+144 -1.71984185e+144 -1.71984185e+144\n",
      " -1.71984240e+144 -1.71984295e+144 -1.71984406e+144 -1.71984185e+144], Bias [-5.51867013e+136 -5.51867513e+136 -5.51868044e+136 -5.51867513e+136\n",
      " -5.51868929e+136 -5.51868752e+136 -5.51868575e+136 -5.51867690e+136\n",
      " -5.51868929e+136 -5.51868044e+136 -5.51868929e+136 -5.51869284e+136\n",
      " -5.51869107e+136 -5.51869638e+136 -5.51869461e+136 -5.51869461e+136\n",
      " -5.51868398e+136 -5.51869815e+136 -5.51867336e+136 -5.51868044e+136\n",
      " -5.51868221e+136 -5.51869461e+136 -5.51868398e+136 -5.51867867e+136\n",
      " -5.51869284e+136 -5.51869284e+136 -5.51869992e+136 -5.51869992e+136\n",
      " -5.51869992e+136 -5.51869992e+136 -5.51868398e+136 -5.51870169e+136\n",
      " -5.51869461e+136 -5.51870169e+136 -5.51869638e+136 -5.51870169e+136\n",
      " -5.51868929e+136 -5.51869992e+136 -5.51869638e+136 -5.51869638e+136\n",
      " -5.51869815e+136 -5.51869992e+136 -5.51870346e+136 -5.51869638e+136]\n",
      "Iteration 14: Cost 9.66851137012174e+304, Weight         [2.55533152e+155 2.55533383e+155 2.55533629e+155 2.55533383e+155\n",
      " 2.55534039e+155 2.55533957e+155 2.55533875e+155 2.55533465e+155\n",
      " 2.55534039e+155 2.55533629e+155 2.55534039e+155 2.55534203e+155\n",
      " 2.55534121e+155 2.55534367e+155 2.55534285e+155 2.55534285e+155\n",
      " 2.55533793e+155 2.55534449e+155 2.55533301e+155 2.55533629e+155\n",
      " 2.55533711e+155 2.55534285e+155 2.55533793e+155 2.55533547e+155\n",
      " 2.55534203e+155 2.55534203e+155 2.55534531e+155 2.55534531e+155\n",
      " 2.55534531e+155 2.55534531e+155 2.55533793e+155 2.55534613e+155\n",
      " 2.55534285e+155 2.55534613e+155 2.55534367e+155 2.55534613e+155\n",
      " 2.55534039e+155 2.55534531e+155 2.55534367e+155 2.55534367e+155\n",
      " 2.55534449e+155 2.55534531e+155 2.55534695e+155 2.55534367e+155], Bias [8.19964860e+147 8.19965602e+147 8.19966391e+147 8.19965602e+147\n",
      " 8.19967706e+147 8.19967443e+147 8.19967180e+147 8.19965865e+147\n",
      " 8.19967706e+147 8.19966391e+147 8.19967706e+147 8.19968233e+147\n",
      " 8.19967970e+147 8.19968759e+147 8.19968496e+147 8.19968496e+147\n",
      " 8.19966917e+147 8.19969022e+147 8.19965338e+147 8.19966391e+147\n",
      " 8.19966654e+147 8.19968496e+147 8.19966917e+147 8.19966128e+147\n",
      " 8.19968233e+147 8.19968233e+147 8.19969285e+147 8.19969285e+147\n",
      " 8.19969285e+147 8.19969285e+147 8.19966917e+147 8.19969548e+147\n",
      " 8.19968496e+147 8.19969548e+147 8.19968759e+147 8.19969548e+147\n",
      " 8.19967706e+147 8.19969285e+147 8.19968759e+147 8.19968759e+147\n",
      " 8.19969022e+147 8.19969285e+147 8.19969811e+147 8.19968759e+147]\n",
      "Iteration 15: Cost inf, Weight         [-3.79671551e+166 -3.79671895e+166 -3.79672260e+166 -3.79671895e+166\n",
      " -3.79672870e+166 -3.79672748e+166 -3.79672626e+166 -3.79672017e+166\n",
      " -3.79672870e+166 -3.79672260e+166 -3.79672870e+166 -3.79673113e+166\n",
      " -3.79672991e+166 -3.79673357e+166 -3.79673235e+166 -3.79673235e+166\n",
      " -3.79672504e+166 -3.79673479e+166 -3.79671773e+166 -3.79672260e+166\n",
      " -3.79672382e+166 -3.79673235e+166 -3.79672504e+166 -3.79672139e+166\n",
      " -3.79673113e+166 -3.79673113e+166 -3.79673601e+166 -3.79673601e+166\n",
      " -3.79673601e+166 -3.79673601e+166 -3.79672504e+166 -3.79673722e+166\n",
      " -3.79673235e+166 -3.79673722e+166 -3.79673357e+166 -3.79673722e+166\n",
      " -3.79672870e+166 -3.79673601e+166 -3.79673357e+166 -3.79673357e+166\n",
      " -3.79673479e+166 -3.79673601e+166 -3.79673844e+166 -3.79673357e+166], Bias [-1.21830505e+159 -1.21830616e+159 -1.21830733e+159 -1.21830616e+159\n",
      " -1.21830928e+159 -1.21830889e+159 -1.21830850e+159 -1.21830655e+159\n",
      " -1.21830928e+159 -1.21830733e+159 -1.21830928e+159 -1.21831006e+159\n",
      " -1.21830967e+159 -1.21831085e+159 -1.21831046e+159 -1.21831046e+159\n",
      " -1.21830811e+159 -1.21831124e+159 -1.21830576e+159 -1.21830733e+159\n",
      " -1.21830772e+159 -1.21831046e+159 -1.21830811e+159 -1.21830694e+159\n",
      " -1.21831006e+159 -1.21831006e+159 -1.21831163e+159 -1.21831163e+159\n",
      " -1.21831163e+159 -1.21831163e+159 -1.21830811e+159 -1.21831202e+159\n",
      " -1.21831046e+159 -1.21831202e+159 -1.21831085e+159 -1.21831202e+159\n",
      " -1.21830928e+159 -1.21831163e+159 -1.21831085e+159 -1.21831085e+159\n",
      " -1.21831124e+159 -1.21831163e+159 -1.21831241e+159 -1.21831085e+159]\n",
      "Iteration 16: Cost inf, Weight         [5.64116576e+177 5.64117087e+177 5.64117630e+177 5.64117087e+177\n",
      " 5.64118535e+177 5.64118354e+177 5.64118173e+177 5.64117268e+177\n",
      " 5.64118535e+177 5.64117630e+177 5.64118535e+177 5.64118897e+177\n",
      " 5.64118716e+177 5.64119259e+177 5.64119078e+177 5.64119078e+177\n",
      " 5.64117992e+177 5.64119440e+177 5.64116906e+177 5.64117630e+177\n",
      " 5.64117811e+177 5.64119078e+177 5.64117992e+177 5.64117449e+177\n",
      " 5.64118897e+177 5.64118897e+177 5.64119621e+177 5.64119621e+177\n",
      " 5.64119621e+177 5.64119621e+177 5.64117992e+177 5.64119802e+177\n",
      " 5.64119078e+177 5.64119802e+177 5.64119259e+177 5.64119802e+177\n",
      " 5.64118535e+177 5.64119621e+177 5.64119259e+177 5.64119259e+177\n",
      " 5.64119440e+177 5.64119621e+177 5.64119983e+177 5.64119259e+177], Bias [1.81015953e+170 1.81016116e+170 1.81016291e+170 1.81016116e+170\n",
      " 1.81016581e+170 1.81016523e+170 1.81016465e+170 1.81016175e+170\n",
      " 1.81016581e+170 1.81016291e+170 1.81016581e+170 1.81016697e+170\n",
      " 1.81016639e+170 1.81016813e+170 1.81016755e+170 1.81016755e+170\n",
      " 1.81016407e+170 1.81016872e+170 1.81016058e+170 1.81016291e+170\n",
      " 1.81016349e+170 1.81016755e+170 1.81016407e+170 1.81016233e+170\n",
      " 1.81016697e+170 1.81016697e+170 1.81016930e+170 1.81016930e+170\n",
      " 1.81016930e+170 1.81016930e+170 1.81016407e+170 1.81016988e+170\n",
      " 1.81016755e+170 1.81016988e+170 1.81016813e+170 1.81016988e+170\n",
      " 1.81016581e+170 1.81016930e+170 1.81016813e+170 1.81016813e+170\n",
      " 1.81016872e+170 1.81016930e+170 1.81017046e+170 1.81016813e+170]\n",
      "Iteration 17: Cost inf, Weight         [-8.38165279e+188 -8.38166038e+188 -8.38166845e+188 -8.38166038e+188\n",
      " -8.38168189e+188 -8.38167920e+188 -8.38167651e+188 -8.38166307e+188\n",
      " -8.38168189e+188 -8.38166845e+188 -8.38168189e+188 -8.38168727e+188\n",
      " -8.38168458e+188 -8.38169265e+188 -8.38168996e+188 -8.38168996e+188\n",
      " -8.38167382e+188 -8.38169534e+188 -8.38165769e+188 -8.38166845e+188\n",
      " -8.38167114e+188 -8.38168996e+188 -8.38167382e+188 -8.38166576e+188\n",
      " -8.38168727e+188 -8.38168727e+188 -8.38169803e+188 -8.38169803e+188\n",
      " -8.38169803e+188 -8.38169803e+188 -8.38167382e+188 -8.38170072e+188\n",
      " -8.38168996e+188 -8.38170072e+188 -8.38169265e+188 -8.38170072e+188\n",
      " -8.38168189e+188 -8.38169803e+188 -8.38169265e+188 -8.38169265e+188\n",
      " -8.38169534e+188 -8.38169803e+188 -8.38170341e+188 -8.38169265e+188], Bias [-2.68953782e+181 -2.68954025e+181 -2.68954284e+181 -2.68954025e+181\n",
      " -2.68954715e+181 -2.68954629e+181 -2.68954543e+181 -2.68954111e+181\n",
      " -2.68954715e+181 -2.68954284e+181 -2.68954715e+181 -2.68954888e+181\n",
      " -2.68954802e+181 -2.68955061e+181 -2.68954974e+181 -2.68954974e+181\n",
      " -2.68954456e+181 -2.68955147e+181 -2.68953939e+181 -2.68954284e+181\n",
      " -2.68954370e+181 -2.68954974e+181 -2.68954456e+181 -2.68954198e+181\n",
      " -2.68954888e+181 -2.68954888e+181 -2.68955233e+181 -2.68955233e+181\n",
      " -2.68955233e+181 -2.68955233e+181 -2.68954456e+181 -2.68955320e+181\n",
      " -2.68954974e+181 -2.68955320e+181 -2.68955061e+181 -2.68955320e+181\n",
      " -2.68954715e+181 -2.68955233e+181 -2.68955061e+181 -2.68955061e+181\n",
      " -2.68955147e+181 -2.68955233e+181 -2.68955406e+181 -2.68955061e+181]\n",
      "Iteration 18: Cost inf, Weight         [1.24534726e+200 1.24534839e+200 1.24534959e+200 1.24534839e+200\n",
      " 1.24535159e+200 1.24535119e+200 1.24535079e+200 1.24534879e+200\n",
      " 1.24535159e+200 1.24534959e+200 1.24535159e+200 1.24535239e+200\n",
      " 1.24535199e+200 1.24535319e+200 1.24535279e+200 1.24535279e+200\n",
      " 1.24535039e+200 1.24535359e+200 1.24534799e+200 1.24534959e+200\n",
      " 1.24534999e+200 1.24535279e+200 1.24535039e+200 1.24534919e+200\n",
      " 1.24535239e+200 1.24535239e+200 1.24535399e+200 1.24535399e+200\n",
      " 1.24535399e+200 1.24535399e+200 1.24535039e+200 1.24535439e+200\n",
      " 1.24535279e+200 1.24535439e+200 1.24535319e+200 1.24535439e+200\n",
      " 1.24535159e+200 1.24535399e+200 1.24535319e+200 1.24535319e+200\n",
      " 1.24535359e+200 1.24535399e+200 1.24535479e+200 1.24535319e+200], Bias [3.99611943e+192 3.99612305e+192 3.99612690e+192 3.99612305e+192\n",
      " 3.99613331e+192 3.99613203e+192 3.99613074e+192 3.99612433e+192\n",
      " 3.99613331e+192 3.99612690e+192 3.99613331e+192 3.99613587e+192\n",
      " 3.99613459e+192 3.99613844e+192 3.99613716e+192 3.99613716e+192\n",
      " 3.99612946e+192 3.99613972e+192 3.99612177e+192 3.99612690e+192\n",
      " 3.99612818e+192 3.99613716e+192 3.99612946e+192 3.99612561e+192\n",
      " 3.99613587e+192 3.99613587e+192 3.99614100e+192 3.99614100e+192\n",
      " 3.99614100e+192 3.99614100e+192 3.99612946e+192 3.99614228e+192\n",
      " 3.99613716e+192 3.99614228e+192 3.99613844e+192 3.99614228e+192\n",
      " 3.99613331e+192 3.99614100e+192 3.99613844e+192 3.99613844e+192\n",
      " 3.99613972e+192 3.99614100e+192 3.99614357e+192 3.99613844e+192]\n",
      "Iteration 19: Cost inf, Weight         [-1.85033889e+211 -1.85034056e+211 -1.85034234e+211 -1.85034056e+211\n",
      " -1.85034531e+211 -1.85034472e+211 -1.85034412e+211 -1.85034115e+211\n",
      " -1.85034531e+211 -1.85034234e+211 -1.85034531e+211 -1.85034650e+211\n",
      " -1.85034590e+211 -1.85034769e+211 -1.85034709e+211 -1.85034709e+211\n",
      " -1.85034353e+211 -1.85034828e+211 -1.85033997e+211 -1.85034234e+211\n",
      " -1.85034294e+211 -1.85034709e+211 -1.85034353e+211 -1.85034175e+211\n",
      " -1.85034650e+211 -1.85034650e+211 -1.85034887e+211 -1.85034887e+211\n",
      " -1.85034887e+211 -1.85034887e+211 -1.85034353e+211 -1.85034947e+211\n",
      " -1.85034709e+211 -1.85034947e+211 -1.85034769e+211 -1.85034947e+211\n",
      " -1.85034531e+211 -1.85034887e+211 -1.85034769e+211 -1.85034769e+211\n",
      " -1.85034828e+211 -1.85034887e+211 -1.85035006e+211 -1.85034769e+211], Bias [-5.93744042e+203 -5.93744579e+203 -5.93745151e+203 -5.93744579e+203\n",
      " -5.93746103e+203 -5.93745913e+203 -5.93745722e+203 -5.93744770e+203\n",
      " -5.93746103e+203 -5.93745151e+203 -5.93746103e+203 -5.93746484e+203\n",
      " -5.93746294e+203 -5.93746865e+203 -5.93746675e+203 -5.93746675e+203\n",
      " -5.93745532e+203 -5.93747056e+203 -5.93744389e+203 -5.93745151e+203\n",
      " -5.93745341e+203 -5.93746675e+203 -5.93745532e+203 -5.93744960e+203\n",
      " -5.93746484e+203 -5.93746484e+203 -5.93747246e+203 -5.93747246e+203\n",
      " -5.93747246e+203 -5.93747246e+203 -5.93745532e+203 -5.93747437e+203\n",
      " -5.93746675e+203 -5.93747437e+203 -5.93746865e+203 -5.93747437e+203\n",
      " -5.93746103e+203 -5.93747246e+203 -5.93746865e+203 -5.93746865e+203\n",
      " -5.93747056e+203 -5.93747246e+203 -5.93747627e+203 -5.93746865e+203]\n",
      "Iteration 20: Cost inf, Weight         [2.74923637e+222 2.74923886e+222 2.74924150e+222 2.74923886e+222\n",
      " 2.74924592e+222 2.74924503e+222 2.74924415e+222 2.74923974e+222\n",
      " 2.74924592e+222 2.74924150e+222 2.74924592e+222 2.74924768e+222\n",
      " 2.74924680e+222 2.74924944e+222 2.74924856e+222 2.74924856e+222\n",
      " 2.74924327e+222 2.74925033e+222 2.74923798e+222 2.74924150e+222\n",
      " 2.74924239e+222 2.74924856e+222 2.74924327e+222 2.74924062e+222\n",
      " 2.74924768e+222 2.74924768e+222 2.74925121e+222 2.74925121e+222\n",
      " 2.74925121e+222 2.74925121e+222 2.74924327e+222 2.74925209e+222\n",
      " 2.74924856e+222 2.74925209e+222 2.74924944e+222 2.74925209e+222\n",
      " 2.74924592e+222 2.74925121e+222 2.74924944e+222 2.74924944e+222\n",
      " 2.74925033e+222 2.74925121e+222 2.74925297e+222 2.74924944e+222], Bias [8.82185813e+214 8.82186611e+214 8.82187461e+214 8.82186611e+214\n",
      " 8.82188876e+214 8.82188593e+214 8.82188310e+214 8.82186894e+214\n",
      " 8.82188876e+214 8.82187461e+214 8.82188876e+214 8.82189442e+214\n",
      " 8.82189159e+214 8.82190008e+214 8.82189725e+214 8.82189725e+214\n",
      " 8.82188027e+214 8.82190291e+214 8.82186328e+214 8.82187461e+214\n",
      " 8.82187744e+214 8.82189725e+214 8.82188027e+214 8.82187177e+214\n",
      " 8.82189442e+214 8.82189442e+214 8.82190574e+214 8.82190574e+214\n",
      " 8.82190574e+214 8.82190574e+214 8.82188027e+214 8.82190857e+214\n",
      " 8.82189725e+214 8.82190857e+214 8.82190008e+214 8.82190857e+214\n",
      " 8.82188876e+214 8.82190574e+214 8.82190008e+214 8.82190008e+214\n",
      " 8.82190291e+214 8.82190574e+214 8.82191141e+214 8.82190008e+214]\n",
      "Iteration 21: Cost inf, Weight         [-4.08481964e+233 -4.08482334e+233 -4.08482727e+233 -4.08482334e+233\n",
      " -4.08483382e+233 -4.08483251e+233 -4.08483120e+233 -4.08482465e+233\n",
      " -4.08483382e+233 -4.08482727e+233 -4.08483382e+233 -4.08483644e+233\n",
      " -4.08483513e+233 -4.08483906e+233 -4.08483775e+233 -4.08483775e+233\n",
      " -4.08482989e+233 -4.08484038e+233 -4.08482202e+233 -4.08482727e+233\n",
      " -4.08482858e+233 -4.08483775e+233 -4.08482989e+233 -4.08482596e+233\n",
      " -4.08483644e+233 -4.08483644e+233 -4.08484169e+233 -4.08484169e+233\n",
      " -4.08484169e+233 -4.08484169e+233 -4.08482989e+233 -4.08484300e+233\n",
      " -4.08483775e+233 -4.08484300e+233 -4.08483906e+233 -4.08484300e+233\n",
      " -4.08483382e+233 -4.08484169e+233 -4.08483906e+233 -4.08483906e+233\n",
      " -4.08484038e+233 -4.08484169e+233 -4.08484431e+233 -4.08483906e+233], Bias [-1.31075304e+226 -1.31075423e+226 -1.31075549e+226 -1.31075423e+226\n",
      " -1.31075759e+226 -1.31075717e+226 -1.31075675e+226 -1.31075465e+226\n",
      " -1.31075759e+226 -1.31075549e+226 -1.31075759e+226 -1.31075843e+226\n",
      " -1.31075801e+226 -1.31075927e+226 -1.31075885e+226 -1.31075885e+226\n",
      " -1.31075633e+226 -1.31075970e+226 -1.31075381e+226 -1.31075549e+226\n",
      " -1.31075591e+226 -1.31075885e+226 -1.31075633e+226 -1.31075507e+226\n",
      " -1.31075843e+226 -1.31075843e+226 -1.31076012e+226 -1.31076012e+226\n",
      " -1.31076012e+226 -1.31076012e+226 -1.31075633e+226 -1.31076054e+226\n",
      " -1.31075885e+226 -1.31076054e+226 -1.31075927e+226 -1.31076054e+226\n",
      " -1.31075759e+226 -1.31076012e+226 -1.31075927e+226 -1.31075927e+226\n",
      " -1.31075970e+226 -1.31076012e+226 -1.31076096e+226 -1.31075927e+226]\n",
      "Iteration 22: Cost inf, Weight         [6.06923132e+244 6.06923681e+244 6.06924265e+244 6.06923681e+244\n",
      " 6.06925239e+244 6.06925044e+244 6.06924850e+244 6.06923876e+244\n",
      " 6.06925239e+244 6.06924265e+244 6.06925239e+244 6.06925629e+244\n",
      " 6.06925434e+244 6.06926018e+244 6.06925823e+244 6.06925823e+244\n",
      " 6.06924655e+244 6.06926213e+244 6.06923486e+244 6.06924265e+244\n",
      " 6.06924460e+244 6.06925823e+244 6.06924655e+244 6.06924071e+244\n",
      " 6.06925629e+244 6.06925629e+244 6.06926408e+244 6.06926408e+244\n",
      " 6.06926408e+244 6.06926408e+244 6.06924655e+244 6.06926602e+244\n",
      " 6.06925823e+244 6.06926602e+244 6.06926018e+244 6.06926602e+244\n",
      " 6.06925239e+244 6.06926408e+244 6.06926018e+244 6.06926018e+244\n",
      " 6.06926213e+244 6.06926408e+244 6.06926797e+244 6.06926018e+244], Bias [1.94751889e+237 1.94752065e+237 1.94752253e+237 1.94752065e+237\n",
      " 1.94752565e+237 1.94752503e+237 1.94752440e+237 1.94752128e+237\n",
      " 1.94752565e+237 1.94752253e+237 1.94752565e+237 1.94752690e+237\n",
      " 1.94752628e+237 1.94752815e+237 1.94752753e+237 1.94752753e+237\n",
      " 1.94752378e+237 1.94752878e+237 1.94752003e+237 1.94752253e+237\n",
      " 1.94752315e+237 1.94752753e+237 1.94752378e+237 1.94752190e+237\n",
      " 1.94752690e+237 1.94752690e+237 1.94752940e+237 1.94752940e+237\n",
      " 1.94752940e+237 1.94752940e+237 1.94752378e+237 1.94753003e+237\n",
      " 1.94752753e+237 1.94753003e+237 1.94752815e+237 1.94753003e+237\n",
      " 1.94752565e+237 1.94752940e+237 1.94752815e+237 1.94752815e+237\n",
      " 1.94752878e+237 1.94752940e+237 1.94753065e+237 1.94752815e+237]\n",
      "Iteration 23: Cost inf, Weight         [-9.01767325e+255 -9.01768141e+255 -9.01769009e+255 -9.01768141e+255\n",
      " -9.01770456e+255 -9.01770167e+255 -9.01769878e+255 -9.01768431e+255\n",
      " -9.01770456e+255 -9.01769009e+255 -9.01770456e+255 -9.01771035e+255\n",
      " -9.01770746e+255 -9.01771614e+255 -9.01771324e+255 -9.01771324e+255\n",
      " -9.01769588e+255 -9.01771903e+255 -9.01767852e+255 -9.01769009e+255\n",
      " -9.01769299e+255 -9.01771324e+255 -9.01769588e+255 -9.01768720e+255\n",
      " -9.01771035e+255 -9.01771035e+255 -9.01772192e+255 -9.01772192e+255\n",
      " -9.01772192e+255 -9.01772192e+255 -9.01769588e+255 -9.01772482e+255\n",
      " -9.01771324e+255 -9.01772482e+255 -9.01771614e+255 -9.01772482e+255\n",
      " -9.01770456e+255 -9.01772192e+255 -9.01771614e+255 -9.01771614e+255\n",
      " -9.01771903e+255 -9.01772192e+255 -9.01772771e+255 -9.01771614e+255], Bias [-2.89362657e+248 -2.89362919e+248 -2.89363197e+248 -2.89362919e+248\n",
      " -2.89363662e+248 -2.89363569e+248 -2.89363476e+248 -2.89363012e+248\n",
      " -2.89363662e+248 -2.89363197e+248 -2.89363662e+248 -2.89363847e+248\n",
      " -2.89363755e+248 -2.89364033e+248 -2.89363940e+248 -2.89363940e+248\n",
      " -2.89363383e+248 -2.89364126e+248 -2.89362826e+248 -2.89363197e+248\n",
      " -2.89363290e+248 -2.89363940e+248 -2.89363383e+248 -2.89363105e+248\n",
      " -2.89363847e+248 -2.89363847e+248 -2.89364219e+248 -2.89364219e+248\n",
      " -2.89364219e+248 -2.89364219e+248 -2.89363383e+248 -2.89364312e+248\n",
      " -2.89363940e+248 -2.89364312e+248 -2.89364033e+248 -2.89364312e+248\n",
      " -2.89363662e+248 -2.89364219e+248 -2.89364033e+248 -2.89364033e+248\n",
      " -2.89364126e+248 -2.89364219e+248 -2.89364405e+248 -2.89364033e+248]\n",
      "Iteration 24: Cost inf, Weight         [1.33984728e+267 1.33984850e+267 1.33984978e+267 1.33984850e+267\n",
      " 1.33985193e+267 1.33985150e+267 1.33985107e+267 1.33984893e+267\n",
      " 1.33985193e+267 1.33984978e+267 1.33985193e+267 1.33985279e+267\n",
      " 1.33985236e+267 1.33985365e+267 1.33985322e+267 1.33985322e+267\n",
      " 1.33985064e+267 1.33985408e+267 1.33984807e+267 1.33984978e+267\n",
      " 1.33985021e+267 1.33985322e+267 1.33985064e+267 1.33984935e+267\n",
      " 1.33985279e+267 1.33985279e+267 1.33985451e+267 1.33985451e+267\n",
      " 1.33985451e+267 1.33985451e+267 1.33985064e+267 1.33985494e+267\n",
      " 1.33985322e+267 1.33985494e+267 1.33985365e+267 1.33985494e+267\n",
      " 1.33985193e+267 1.33985451e+267 1.33985365e+267 1.33985365e+267\n",
      " 1.33985408e+267 1.33985451e+267 1.33985537e+267 1.33985365e+267], Bias [4.29935482e+259 4.29935871e+259 4.29936285e+259 4.29935871e+259\n",
      " 4.29936975e+259 4.29936837e+259 4.29936699e+259 4.29936009e+259\n",
      " 4.29936975e+259 4.29936285e+259 4.29936975e+259 4.29937251e+259\n",
      " 4.29937113e+259 4.29937527e+259 4.29937389e+259 4.29937389e+259\n",
      " 4.29936561e+259 4.29937665e+259 4.29935733e+259 4.29936285e+259\n",
      " 4.29936423e+259 4.29937389e+259 4.29936561e+259 4.29936147e+259\n",
      " 4.29937251e+259 4.29937251e+259 4.29937803e+259 4.29937803e+259\n",
      " 4.29937803e+259 4.29937803e+259 4.29936561e+259 4.29937941e+259\n",
      " 4.29937389e+259 4.29937941e+259 4.29937527e+259 4.29937941e+259\n",
      " 4.29936975e+259 4.29937803e+259 4.29937527e+259 4.29937527e+259\n",
      " 4.29937665e+259 4.29937803e+259 4.29938079e+259 4.29937527e+259]\n",
      "Iteration 25: Cost inf, Weight         [-1.99074716e+278 -1.99074896e+278 -1.99075088e+278 -1.99074896e+278\n",
      " -1.99075407e+278 -1.99075343e+278 -1.99075279e+278 -1.99074960e+278\n",
      " -1.99075407e+278 -1.99075088e+278 -1.99075407e+278 -1.99075535e+278\n",
      " -1.99075471e+278 -1.99075663e+278 -1.99075599e+278 -1.99075599e+278\n",
      " -1.99075215e+278 -1.99075726e+278 -1.99074832e+278 -1.99075088e+278\n",
      " -1.99075152e+278 -1.99075599e+278 -1.99075215e+278 -1.99075024e+278\n",
      " -1.99075535e+278 -1.99075535e+278 -1.99075790e+278 -1.99075790e+278\n",
      " -1.99075790e+278 -1.99075790e+278 -1.99075215e+278 -1.99075854e+278\n",
      " -1.99075599e+278 -1.99075854e+278 -1.99075663e+278 -1.99075854e+278\n",
      " -1.99075407e+278 -1.99075790e+278 -1.99075663e+278 -1.99075663e+278\n",
      " -1.99075726e+278 -1.99075790e+278 -1.99075918e+278 -1.99075663e+278], Bias [-6.38798802e+270 -6.38799380e+270 -6.38799995e+270 -6.38799380e+270\n",
      " -6.38801020e+270 -6.38800815e+270 -6.38800610e+270 -6.38799585e+270\n",
      " -6.38801020e+270 -6.38799995e+270 -6.38801020e+270 -6.38801430e+270\n",
      " -6.38801225e+270 -6.38801840e+270 -6.38801635e+270 -6.38801635e+270\n",
      " -6.38800405e+270 -6.38802045e+270 -6.38799175e+270 -6.38799995e+270\n",
      " -6.38800200e+270 -6.38801635e+270 -6.38800405e+270 -6.38799790e+270\n",
      " -6.38801430e+270 -6.38801430e+270 -6.38802250e+270 -6.38802250e+270\n",
      " -6.38802250e+270 -6.38802250e+270 -6.38800405e+270 -6.38802455e+270\n",
      " -6.38801635e+270 -6.38802455e+270 -6.38801840e+270 -6.38802455e+270\n",
      " -6.38801020e+270 -6.38802250e+270 -6.38801840e+270 -6.38801840e+270\n",
      " -6.38802045e+270 -6.38802250e+270 -6.38802660e+270 -6.38801840e+270]\n",
      "Iteration 26: Cost inf, Weight         [2.95785520e+289 2.95785788e+289 2.95786072e+289 2.95785788e+289\n",
      " 2.95786547e+289 2.95786452e+289 2.95786357e+289 2.95785882e+289\n",
      " 2.95786547e+289 2.95786072e+289 2.95786547e+289 2.95786737e+289\n",
      " 2.95786642e+289 2.95786926e+289 2.95786832e+289 2.95786832e+289\n",
      " 2.95786262e+289 2.95787021e+289 2.95785693e+289 2.95786072e+289\n",
      " 2.95786167e+289 2.95786832e+289 2.95786262e+289 2.95785977e+289\n",
      " 2.95786737e+289 2.95786737e+289 2.95787116e+289 2.95787116e+289\n",
      " 2.95787116e+289 2.95787116e+289 2.95786262e+289 2.95787211e+289\n",
      " 2.95786832e+289 2.95787211e+289 2.95786926e+289 2.95787211e+289\n",
      " 2.95786547e+289 2.95787116e+289 2.95786926e+289 2.95786926e+289\n",
      " 2.95787021e+289 2.95787116e+289 2.95787306e+289 2.95786926e+289], Bias [9.49128246e+281 9.49129104e+281 9.49130018e+281 9.49129104e+281\n",
      " 9.49131541e+281 9.49131236e+281 9.49130932e+281 9.49129409e+281\n",
      " 9.49131541e+281 9.49130018e+281 9.49131541e+281 9.49132150e+281\n",
      " 9.49131846e+281 9.49132759e+281 9.49132455e+281 9.49132455e+281\n",
      " 9.49130627e+281 9.49133064e+281 9.49128800e+281 9.49130018e+281\n",
      " 9.49130323e+281 9.49132455e+281 9.49130627e+281 9.49129714e+281\n",
      " 9.49132150e+281 9.49132150e+281 9.49133368e+281 9.49133368e+281\n",
      " 9.49133368e+281 9.49133368e+281 9.49130627e+281 9.49133673e+281\n",
      " 9.49132455e+281 9.49133673e+281 9.49132759e+281 9.49133673e+281\n",
      " 9.49131541e+281 9.49133368e+281 9.49132759e+281 9.49132759e+281\n",
      " 9.49133064e+281 9.49133368e+281 9.49133978e+281 9.49132759e+281]\n",
      "Iteration 27: Cost inf, Weight         [-4.39478582e+300 -4.39478979e+300 -4.39479402e+300 -4.39478979e+300\n",
      " -4.39480107e+300 -4.39479966e+300 -4.39479825e+300 -4.39479120e+300\n",
      " -4.39480107e+300 -4.39479402e+300 -4.39480107e+300 -4.39480389e+300\n",
      " -4.39480248e+300 -4.39480672e+300 -4.39480531e+300 -4.39480531e+300\n",
      " -4.39479684e+300 -4.39480813e+300 -4.39478838e+300 -4.39479402e+300\n",
      " -4.39479543e+300 -4.39480531e+300 -4.39479684e+300 -4.39479261e+300\n",
      " -4.39480389e+300 -4.39480389e+300 -4.39480954e+300 -4.39480954e+300\n",
      " -4.39480954e+300 -4.39480954e+300 -4.39479684e+300 -4.39481095e+300\n",
      " -4.39480531e+300 -4.39481095e+300 -4.39480672e+300 -4.39481095e+300\n",
      " -4.39480107e+300 -4.39480954e+300 -4.39480672e+300 -4.39480672e+300\n",
      " -4.39480813e+300 -4.39480954e+300 -4.39481236e+300 -4.39480672e+300], Bias [-1.41021621e+293 -1.41021749e+293 -1.41021884e+293 -1.41021749e+293\n",
      " -1.41022111e+293 -1.41022065e+293 -1.41022020e+293 -1.41021794e+293\n",
      " -1.41022111e+293 -1.41021884e+293 -1.41022111e+293 -1.41022201e+293\n",
      " -1.41022156e+293 -1.41022292e+293 -1.41022246e+293 -1.41022246e+293\n",
      " -1.41021975e+293 -1.41022337e+293 -1.41021703e+293 -1.41021884e+293\n",
      " -1.41021930e+293 -1.41022246e+293 -1.41021975e+293 -1.41021839e+293\n",
      " -1.41022201e+293 -1.41022201e+293 -1.41022382e+293 -1.41022382e+293\n",
      " -1.41022382e+293 -1.41022382e+293 -1.41021975e+293 -1.41022427e+293\n",
      " -1.41022246e+293 -1.41022427e+293 -1.41022292e+293 -1.41022427e+293\n",
      " -1.41022111e+293 -1.41022382e+293 -1.41022292e+293 -1.41022292e+293\n",
      " -1.41022337e+293 -1.41022382e+293 -1.41022473e+293 -1.41022292e+293]\n",
      "Iteration 28: Cost inf, Weight         [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf], Bias [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf]\n",
      "Iteration 29: Cost inf, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 30: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 31: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 32: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 33: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 34: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 35: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 36: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 37: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 38: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 39: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 40: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 41: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 42: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 43: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 44: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 45: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 46: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 47: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 48: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 49: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 50: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 51: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 52: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 53: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 54: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 55: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 56: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 57: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 58: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 59: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 60: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 61: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 62: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 63: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 64: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 65: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 66: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in square\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in multiply\n",
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: overflow encountered in multiply\n",
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in subtract\n",
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:40: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 67: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 68: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 69: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 70: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 71: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 72: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 73: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 74: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 75: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 76: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 77: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 78: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 79: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 80: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 81: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 82: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 83: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 84: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 85: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 86: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 87: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 88: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 89: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 90: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 91: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 92: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 93: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 94: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 95: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 96: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 97: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 98: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 99: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "Iteration 100: Cost nan, Weight         [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan], Bias [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-176-eb1947d27ecd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mdes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-176-eb1947d27ecd>\u001b[0m in \u001b[0;36mdes\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mestimated_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meatimated_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Estimated Weight: {estimated_weight}\\nEstimated Bias: {eatimated_bias}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-173-80513c66baa3>\u001b[0m in \u001b[0;36mgradient_descent\u001b[1;34m(x, y, iterations, learning_rate, stopping_threshold)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m# Visualizing the weights and cost at for all iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'o'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'red'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cost vs Weights\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2811\u001b[0m     return gca().plot(\n\u001b[0;32m   2812\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m-> 2813\u001b[1;33m         is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1808\u001b[0m                         \u001b[1;34m\"the Matplotlib list!)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1810\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1611\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1612\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1613\u001b[0m             \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1614\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36madd_line\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   1893\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1896\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1897\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_line%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_update_line_limits\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mFigures\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdating\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m         \"\"\"\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mget_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    943\u001b[0m         \"\"\"\n\u001b[0;32m    944\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 945\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    946\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mrecache\u001b[1;34m(self, always)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0malways\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m             \u001b[0mxconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_xunits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xorig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_to_unmasked_float_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxconv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    641\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\matplotlib\\cbook\\__init__.py\u001b[0m in \u001b[0;36m_to_unmasked_float_array\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#def des():   \n",
    "    X1=\n",
    "    estimated_weight, eatimated_bias = gradient_descent(X, y, iterations=100)\n",
    "    print(f\"Estimated Weight: {estimated_weight}\\nEstimated Bias: {eatimated_bias}\")\n",
    " \n",
    "    # Making predictions using estimated parameters\n",
    "    Y_pred = estimated_weight*X + eatimated_bias\n",
    " \n",
    "    # Plotting the regression line\n",
    "    plt.figure(figsize = (8,6))\n",
    "    plt.scatter(X, y, marker='o', color='red')\n",
    "    plt.plot([min(X), max(X)], [min(Y_pred), max(Y_pred)], color='blue',markerfacecolor='red',\n",
    "             markersize=10,linestyle='dashed')\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.show()\n",
    "    \n",
    "des()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEWCAYAAACEz/viAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XmcHHWd//HXh0mABJFgEtkAyQxgFOVKMCI+0BUhrojcl+CAXBLklA2sHEGu/UVQBORYhCiRuIwIPw4BjauQBbkUSSBchmDAhCtASAgQkkAy89k/vtWkp4+Z6pmururu9/PxmMd0fbuq61PTSX+6vqe5OyIi0tzWSjsAERFJn5KBiIgoGYiIiJKBiIigZCAiIigZiIgISgYiIoKSgaTEzOab2QozW5b3s7GZtZmZ55XNN7MzCo51M/tEQdl5ZnZDwjG7mb2XF9vSqHxnM+uKyt41s7lmdmTecblrGlDweteb2f9LMmaRuAb0votIYvZ093vyC8ysLXo4xN1Xm9k44M9mNsvd7651gCVs5+7zSpS/6u6bmpkBXwfuNLOH3X1ujeOLxcwGuPvqtOOQ7NCdgWSau88EngHG9OV4M1vHzJaa2dZ5ZcOju5KPm9kwM/tdtM8SM3vAzPr8/8KD6cASYNu+vo6Z7WBmM83sHTN73cwuzXvui2b2cBTzS2Z2RFS+gZn9yswWmdkCMzs7dy1mdoSZPWRml5nZEuC8qPwoM5tjZm+Z2R/NrLWvMUt9UzKQTDOzHYGtgVLfxnvl7u8DtwGH5BUfBPzZ3d8ATgVeBoYDGwFnAX2eo8XM1jKzvYBhfY05cjlwubt/FNgCuDl6/VHAH4Aro5jHALOjY64ENgA2B74MfBs4Mu81Pw+8AHwcmGxm+xCud7/otR4AbuxHzFLHlAwkTb+Nvt0uNbPfFjz3ppmtAP4CXA0UPl+JX9M9GXwrKgNYBYwAWt19lbs/4D1P2PVYXsxX5JVvHLUhrABuBya6++P9iHkV8AkzG+buy9z9r1F5O3CPu98YxbvY3WebWQvwTeBMd3/X3ecDlwCH5b3mq+5+pbuvdvcVwLHAhe4+J6oy+iEwRncHzUnJQNK0j7sPiX72KXhuGPAR4DRgZ2Bg3nOdBdtE26vKnOd/gUFm9vnog24M4QMb4GLCN/g/mdkLhY3VJWyfF/PJeeWvuvsQ4KPAFcAuec/l6uYriflo4JPAs2b2qJntEZWPBJ4vsf8wYG1gQV7ZAmCTvO2XCo5pBS7PJTdC1ZYVHCNNQslAMsvdO939EmAlcHzeUy8CbQW7b0b3D8L81+kiVLMcQrgr+J27vxs99667n+rumwN7AhPNbNd+xPw+cDqwTVQNA7CQ8KFfScz/cPdDCFU6PwJuMbP1CB/oW5Q45M3oHPnf6kcBr+S/bMExLwHH5iW3Ie4+yN0f7uUypQEpGUg9uAj4vpmtG23fBJxtZptGdfTjCR/kt/TwGr8mVKO0s6aKCDPbw8w+EfUCeodw19HZn2Dd/QNCFc050XYncCuhnn6omQ00s0OAzxDq/4uY2aFmNjxKZEuj4k6gAxhvZgeZ2YDo9cZE57g5Osf60R3QRKCn7rbXAGea2VbROTcwswP7c+1Sv5QMpB78HngLOCbavgB4GHgwKv8x0O7uT5d7AXd/BHgP2JjuH8CjgXuAZUTtE+5+XxVingqMMrM9o+3jCdUwTwJvACcC33D318scvxvwjJktIzQmH+zuK939RWB3QsP3EkLj8XbRMSdF1/gC4W/z6yiOktz9dsJdx2/M7B3gaUK3WGlCpsVtREREdwYiIqJkICIiSgYiIoKSgYiIUEcT1Q0bNszb2trSDkNEpK7MmjXrTXcf3tt+dZMM2tramDlzZtphiIjUFTMrObCxkKqJREREyUBERJQMREQEJQMREUHJQEREUDIQERGUDEREBCUDEZFsevNN+Pd/h8WLa3I6JQMRkSxZvRquugo++Um48kq4996anFbJQEQkK+69F8aOhZNOgu23hyeegAMOqMmplQxERNK2YAEcdBDssgssWwa33QZ33w1bbVWzEOpmbiIRkYazYgX8+Mdw0UVgBhdcAKedBoMG1TwUJQMRkVpzD9/+Tz013BV885shKYwalVpIqiYSEamlp5+G8eNDW8AGG8B998FvfpNqIgAlAxGR2njrLTj5ZBgzBmbPhquvhlmz4MtfTjsyQNVEIiLJ6uyE666Ds84KCeG73w1tA0OHph1ZN7ozEBFJyoMPwuc+B8ceG3oGPfYY/Nd/ZS4RgJKBiEj1vfIKtLfDl74EixbBTTeFtoHttks7srJUTSQiUi0rV8Kll8IPfxhGEv/gB3D66bDeemlH1islAxGR/nKHu+4Kcwm98ALsuy9ccglstlnakcWmaiIRkf549ln4+tdh771h3XXDyOHbbqurRABKBiIiffP222HQ2DbbwF//Cj/9aegyOn582pH1iaqJREQq0dUF06bBGWeExuHvfAcmT4bhw9OOrF+UDERE4nrkkTCj6KOPwhe+ANOnw2c/m3ZUVaFqIhGR3rz2GhxxBOy4I7z8MtxwAzz0UMMkAlAyEBEp74MP4Cc/CQvN3HhjqBqaOzeMITBLO7qqUjWRiEgpf/gDnHIKPPcc7LFHGD8wenTaUSVGdwYiIvnmzYM994Tddw/b06eHMQQNnAhAyUBEJHj33VANtNVWYeqIiy+Gp54KYwiagKqJRKS5uUNHB3z/+7BwIRx+OFx4IYwYkXZkNZXonYGZrWtmfzOzJ8zsGTM7PyrfzMweMbN/mNlNZrZ2knGIiJQ0axZ88Ytw2GGw6aZh8Nj11zddIoDkq4neB3Zx9+2AMcBuZrYj8CPgMncfDbwFHJ1wHCIia7zxBhxzTJheet48mDo1JILPfz7tyFKTaDLwYFm0OTD6cWAX4JaofBqwT5JxiIgAsGoVXH556Cp6/fVhYrnnnoMjj4S1mrsJNfGrN7MWM5sNvAHcDTwPLHX31dEuLwOblDl2gpnNNLOZixYtSjpUEWlk99wTlpw85ZRwB/DUU2Fm0Q02SDuyTEg8Gbh7p7uPATYFdgA+XWq3MsdOcfdx7j5ueJ3P+yEiKfnnP2G//eCrXw3rDdxxB/zP/8CWW6YdWabU7L7I3ZcC9wE7AkPMLNeTaVPg1VrFISJN4r334Jxz4NOfhj/+MSw488wzsNdeDTd6uBqS7k003MyGRI8HAeOBOcC9wAHRbocDdyQZh4g0EfewzOSWW8J//ifsv3+YQuLMM8N6A1JS0ncGI4B7zexJ4FHgbnf/HXA6MNHM5gFDgesSjkNEmsETT8DOO8PBB4cppR94IIwh2HTTtCPLvEQHnbn7k8DYEuUvENoPRET6b/HisN7wtdfChhuG30cfDS0taUdWNzQCWUTq1+rVMGUKnH02vPMOnHACnH9+SAhSkebuWCsi9evPfw7rCZxwQugyOns2XHFFwySCjg5oawvDH9rawnaSlAxEpL68+CJ885uhbWDpUrjlFpgxA7beOu3IqqajAyZMgAULQnv4ggVhO8mEoGQgIvVhyRKYODH0ErrzTjjvPJgzJ/QWarCuopMmwfLl3cuWLw/lSVGbgYhkmzsMHhwGjAEceGCYXrq1Nd24EvTii5WVV4PuDEQku847L1Sa5xLBnnvCzTc3dCIAGDWqsvJq0J2BiGTP44/D9tt3L1u2DNZbL514amzy5NBGkF9VNHhwKE+K7gxEJDvefz/U/+cnggcfDFVFTZIIANrbQ4/Z1tbw52htDdvt7cmdU8lARLJhn326Txdx8skhCey0U3oxpai9HebPh66u8DvJRACqJhKRtN1xR0gE+To7m359gVpTMhCRdCxaBB//ePeyF16AzTZLJ54mp9QrIrXlHtYYzk8E114bypUIUqNkICK1c/HFofrntdfC9rbbhiQwYUK6cYmqiUSkBv7+d9hqq+5lS5dqyckM0Z2BiCRn1arQNzI/EdxzT7gbUCLIFCUDEUnGt78Na6+9Zvuoo0IS2HXX9GKSslRNJCLV9ac/wde+1r1s1SoYoI+bLNOdgYhUpOw8+0uXhiqh/ETw7LPhbkCJIPOUDEQktnLz7L+9yWe6Lypz6aVhh099Kr1gpSJK1yISW+E8+8dxNVcvPwFyZa2tYe4EqTtKBiISW24+/U/zd/5OQVfRN9+EoUNrH5RUhaqJRCS2zUeuwrFuiWAP7qKt1ZUI6pzuDEQknnXXZd7773crMpzBg2FKgvPsS23ozkBEenbttaGXUF4i2HLke6xlXpN59qU2dGcgIqW9+SYMH9697K67YI89eDadiCRBSgYiUsys+/aYMWEpSmlYqiYSyaiyg7uSdMQRxYmgqyv1RJDK36LJ6M5AJINyg7tyffpzg7sgofr5UgvQz5kDW26ZwMkqU/O/RZMyd087hljGjRvnM2fOTDsMkZpoawsfeoWqPqarqwtaWrqXTZwIl1xSxZP0T83+Fg3KzGa5+7je9tOdgUgG5QZ3xS3vk5Ej4eWXu5dl8MthTf4WojYDkSwaNaqy8or8+tehXSA/ESxdmslEAAn/LeRDSgYiGTR5Mgwe3L1s8OBQ3mdvvx2SQH5Fe0dH5heaSeRvIUWUDEQyqL09DOZqbQ2f3/0e3GUGQ4as2R45MiSBb32rKvEmqep/CylJDcgijWziRLjssu5lnZ2hj6Y0hbgNyIn+izCzkWZ2r5nNMbNnzOx7Ufl5ZvaKmc2OfnZPMg6RrEm83/yzz4av0fmJ4PHHw92AEoGUkHRvotXAqe7+mJmtD8wys7uj5y5z958kfH6RzEm033ypD/sjj4SpU/v5wtLoEv2K4O4L3f2x6PG7wBxgkyTPKZJ1hQvEQNieNKmfLzx2bHEicFcikFhqdr9oZm3AWOCRqOhEM3vSzKaa2YZljplgZjPNbOaiRYtqFKlIsqreb/6uu0KV0OzZa8oWLcpsV1HJppokAzP7CHArcIq7vwP8DNgCGAMsBEoOd3T3Ke4+zt3HDS+cPVGkTlWt3/zy5SEJ7LXXmrJrrglJYNiwPscnzSnxZGBmAwmJoMPdbwNw99fdvdPdu4CfAzskHYdIVlSl37wZrLfemu111glJ4NhjqxKjNJ+kexMZcB0wx90vzSsfkbfbvsDTScYhkiX96jd//vnFs4quWgUrVyYSqzSPpHsT7QQcBjxlZrkKzbOAQ8xsDODAfEBfZ6SptLdX2HNo/nzYbLPuZQ8+CDvtVM2wpIklmgzc/UHASjw1PcnzijSUwjuBffaB229PJxZpWBp9IpIxuQFp99j44kTgrkQgiVAyEMmQjg6YcVQH8xcY45nxYfltV7ysrqKSqIqSgZl90cyOjB4PN7PNejtGRGJasYL2Q42pHxz6YdFpXIzhTLxEYzUlWbHbDMzsXGAc8Cngl8BA4AZCI7GI9EdhdRBgrLkT0EIukrRK7gz2BfYC3gNw91eB9ZMISqRpHHNMUSJYj2XdEgFoIRdJXiW9iT5wdzczBzCz9Xo7QETKeOEF2GKL7mW//CUdA4+ACUDe3EVayEVqoZJkcLOZXQsMMbNjgKMIo4dFpBIlqoRyjcO5oQeTJoWqoVGjQiLQQi6StIoWtzGzrwL/Rhg78Ed3v7uXQ6pGi9tI3Rs6FJYs6V7W1VU6OYhUSdzFbWLdGZhZC+HDfzxQswQg0hDuuqv7ZHIQFpoZMyadeERKiNWA7O6dwHIzy+6q2ZJ5ia/ulbHzsmpV8ayiX/sauNPxzJh0YhIpo5I2g5WEOYbuJupRBODuJ1c9Kmk4ia7ulcHz9tQukFpMIj2I3WZgZoeXKnf3aVWNqAy1GdS3trbwoVeotTXMwdYw5z3jDPjRj7qXvfUWDBmSXkzS1KraZgDhQ9/M1gY+GRXNdfdVfQ1QmkvVV/fK2nkXLoSNN+5edsklMHFiejGJVKCSEcg7A9MIU04bMNLMDnf3+5MJTRrJqFGlvw0nPZiqJuftoUootZhEKlTJCORLgH9z9y+7+78CXwMuSyYsaTRVWd0ra+fdeuviRNDZ2euEcmn9LUR6UkkyGOjuc3Mb7v4cYX4ikV71a3WvrJ33vvvCiz3zzJqyBx4ISWCt3v9LpfW3EOlJJQ3IUwkrk/13VNQODHD3IxOKrRs1IEvqurqgpaV72dix8Nhj6cQjEkPcBuRK7gyOA54BTga+B/wd+G7fwhMpltp4gDjMihOBuxKBNIxKksEA4HJ338/d9wWuAFp6OUYkllzf+wULwmdsru996gnhxz8ubhd47TUtNCMNp5JkMAMYlLc9CLinuuFIs5o0ac0grJzly0N5KpYsCUng9NPXlJ19dkgCG22UUlAiyalkBPK67r4st+Huy8xscE8HiMSVqb73FXYVFWkEldwZvGdm2+c2zOyzwIrqhyTNqFwf+5r2vf/KV4oTwapVSgTSFCpJBqcA/9/MHjCzB4CbgBOTCUuaTap972fNCkngvvvWlE2fHpLAgEpunkXqVyXTUTxqZlsS1kA24FlNRyHVkutjX9NFXUqNCxgxAl59NcGTimRT7DsDMzuQ0G7wNLA3cFN+tZFIf7W3h4naurrC78RnFS1MBO5KBNK0Kqkm+oG7v2tmXyRMRTEN+FkyYYkk5Oc/L24XmD9f7QLS9CpJBp3R728AP3P3O4C1qx+SZElfB4Idf3yobjcLv48/PskoY1i2LASTWzgA4LjjQhJobU0vLpGMqKR17BUzuxYYD/zIzNahsmQidaavi7Acfzz8LO+esbNzzfbVVycTa4/UVVSkV5XMTTQY2A14yt3/YWYjgG3c/U/R8xu6+1tJBaq5iWqvr4uwDBgQEkChlhZYvbpa0cWw5ZYwd273spUrYZ11ahiESLqqPjeRuy9399vc/R/R9sJcIojM6EOckmF9HQhWKhH0VF51ua6i+Yngpz8NdwNKBCIlVbMTdYl7calnfV2EpaWl/J1B4lQlJNIn1azz1/+4BtPXgWD5bbRxyqvCrDgRuCsRiMSkBmApq6+LsFx9deiok7sTaGkJ24k0Hl9xRXES+NvflAREKhS7AbnXFzJ73N3HVuXFSlADsnSzYkXxbcvo0fDcc+nEI5JRcRuQe20zMLOPFRQ5sNSLs8iuJY4dCfwK+BegC5ji7pdHr3kT0AbMBw5KsieSNBi1C4hUXZxqolnAzOj3LOAx4A0zu8fM2nI7ufuSEseuBk51908DOwInmNlngDOAGe4+mtAL6Yz+XITUp4oHtO2yS3EieOstJQKRKug1Gbj7Zu6+efQ79zMcuBq4ppdjF7r7Y9Hjd4E5wCaEuY2mRbtNA/bpz0VI/aloZbO5c0MSuPfeNWWTJoUDhwypWcwijaxfbQZm9pi7x5qsLrqLuB/YGnjR3YfkPfeWu2/Y0/FqM2gssQe0qUpIpF+qPuisxAk+Evf4aN9bgVPc/Z0KzjHBzGaa2cxFixb1MVLJol4HtJXqKtrVpUQgkpA4DcgTSxRvCOwFXBXj+IGERNDh7rdFxa+b2Qh3XxhNa/FGqWPdfQowBcKdQW/nkvpRbkDbyR+7Aeyw7oUzZoT2AhFJTJwRyOsXbDvwGnCouz/V04FmZsB1wBx3vzTvqTuBw4GLot93xI5YGsLkyd0nwRvAKlaxNizO22nQoDU7iEiiek0G7n5+uefMrNXdS3y/+9BOwGHAU2Y2Oyo7i5AEbjazo4EXgQPjhyyNIH9ls/kL1C4gkrZYcxOZ2RcIvYDud/c3zGxbQnfQLwEjyx3n7g9Sfs6ionEJ0lzaL9+B9gWPdi988UUYWfaflIgkpNcGYDO7GJgK7A/83szOBe4GHgFGJxuepKGvC9rE9vzzoXH40bxEMH58uBtQIhBJRZw7g28AY919pZltCLwKbJubyloaS18XtIlNXUVFMilO19AV7r4SIJoyYq4SQeOaNKm4zXb58lDeL6W6iq5erUQgkhFxksEWZnZn7gdoK9iWBtLXBW3K+v3vi5PAL34RkkBNFjgQkTjiVBPtXbB9SRKBSDb0dUGbIl1dpT/sdScgkklxksE/3b2v3wulzhT2/4d4C9p0o3YBkboTp5rot7kHZnZrgrFIBvR1QRsAxo4tTgRz5yoRiNSBOHcG+f+7N08qEMmO9vYKew699FJxPdJaa5VeCFlEMilOMvAyj0VUJSTSIOJUE21nZu+Y2bvAttHjd8zsXTOLPQOp9F8Sg8Eqfc3c/qW6iv5m6nIlApE6FWdxmxZ3/6i7r+/uA6LHue2P1iJIqXAxmIRes6MDHjrqF0VzCV3GKRjO0ScOqv5oZRGpiX4tblNLzb64TezFYJJ6Tfdw+1DACmoO+xOPiFRf3MVtYk1UJ+mr+mCwSl6zRLtAYRKoRjwikp4+r3QmtVVu0FfFg8Eqec1x44oSwdeZXjYR9DceEUmPkkGdmDw5DP7KV/FgsJivefGZS0ISmDWr23MdNzj3D/562dfrbzwikh4lgzrRr8FgFbzme8uNA787tPuO7uBetP/QoeGnWvGISHrUgCxBqfECixfDxz5W+1hEpGriNiDrzqCBxRpDMGNGcSI48MBwN6BEINI01JuoQcVapEajh0UkojuDBtXjIjWlFpqJ2gVEpDkpGTSoUv39r+TEotHD3HqrkoCIqJqoUeUvUjOI5SxnveKdlAREJKI7gwaVG0PgWHEiUJWQiBRQMmhQ7Yca7y3vXiV025WvKAmISElKBo3m4YeLG4dHjAB39jtx43RiEpHMU5tBI1FXURHpI90Z1ED+4K9hw8JPJQvU9Dp4rFRX0c5OJQIRiU3JIGGFC8gsXhx+4i5Q0+MCNEccUZwEzjyz7NoDIiLlaG6ihJVbQCZfTwvClDp+IB/wAesU71wn76WI1I4Wt8mIOIu99LRP4XOO2gVEpPpUl5CwOIu99LRP7jnHihPBrFlKBCJSFUoGCSu1gEy+3haEufykeSXvBjpucNh++ypEKCKiaqLE5WYInTQpVPnkZoVesiR86588uYcFYczYu6CordV7PkZEpA+UDGqgvb3CD+9S4wU++AAGDmR+tYISEcmjaqIs+cUvihPB+eeHdoGBA9OJSUSaQqLJwMymmtkbZvZ0Xtl5ZvaKmc2OfnZPMoZai7O6WG4fMxgwAMw8bBxzTPcd3eGcc6p6bhGRUpKuJroeuAr4VUH5Ze7+k4TPXXNxVhcr3Gd1Z3W6isZa2UxEpIxE7wzc/X5gSZLnyJIeVxcr2Ocu9ijqJfQ5/kZba9+6isY5t4hIOWk1IJ9oZt8GZgKnuvtbpXYyswnABIBRcTrsp6zc4LH88mULFuMMK9rHCEnAYgxS6+u5RUTKSaMB+WfAFsAYYCFwSbkd3X2Ku49z93HDhw+vVXx9Vi5ffVhuxpsFicCi4WS9vUa/zy0i0oOaJwN3f93dO929C/g5sEOtY0hKqQFmgwfD86+sU9RLaBDLuyWB3L49DUDry7n7+noi0lxqngzMbETe5r7A0+X2rTft7TBlSph4zgwO2OgB3ltutKz+YM1OV11Fxw3ORq2DAGhpCcWtreHYvjb2Fp67v68nIs0l0VlLzexGYGdgGPA6cG60PQZwYD5wrLsv7O216m7WUi00IyIZEHfW0qR7Ex3i7iPcfaC7b+ru17n7Ye6+jbtv6+57xUkEtdSfvvodHZReaMadjhucYcPWPD1sGBx/vMYFiEg2aD2DPIV99SHUu8epbnlyv/PY9vbzu5X96zqPcOx1oUnkqKPCjBI9iXsuEZG44t4ZKBnkKbcQTU+Lz7BsGay/flFxrnG4tTVs97bATaxziYhUSIvb9EHFffVLtAsU9hCqtJ+/xgWISBo0UV2e2H31R48uSgTbjFxalAhyx1bS11/jAkQkDUoGeXrtq/+Xv4QkMG/emh3+4z/AnTMu3KDssZMnw9pr935+jQsQkbSomihP4UI03Raf6aWraI/HRr73PVi8ODweOhQOOgimTy+/v4hIragBuTelkkBXV+lyEZGMycQ4g7p27rnFH/i33BLuBpQIRKTBKBkUev/98GF/wQXdy91h//3LDkrTwjIiUs/UZpCvl3aBcgvIPPQQTJumhWVEpH7pzgDgK18pTgSvvlo0l1C5BWSmTNHCMiJS35o7Gbz8ckgC9923puzQQ0MSGDGiaPdyA8I6O0uXawCZiNSL5q0m6sOsoqNGlZ5WoqWldELQADIRqRfNd2dw4YXFiaCzM9b00uUGpU2YoIVlRKS+NU8ymDcvJIGzzlpT9uijIQmsFe/PUG4Bmauv1sIyIlLfmmPQ2VVXwUknrdm+6CI4/fTqBCYikmGatTTfX/4Sfq+zDqxcmW4sIiIZ1BzVRB0doTooLxF0dFBy5bHCso6O7gPKhg0LPxpcJiKNpDmqiQp0dMCRR8KqVb3vO2BASAzl9tXqZCKSZZqbqAeTJsVLBACrV/e8rwaXiUgjaMpkUO3BYBpcJiL1rimTQbUHg2lwmYjUu6ZMBpMnw8CB8fYdMKDnfTW4TEQaQVMmg/Z2+OUvw2pjOUOHwnHHFZddf33YNzegbOjQ8KPBZSLSSJqyN5GISLNQb6I85cYJaMyAiEjQ8COQCxekyS1IX/hYC9KISDNr+DuDUgvSlKMxAyLSrBo+GVQ6BkBjBkSkGTV8Mqh0DIDGDIhIM2r4ZFBqQZpyNGZARJpVwyeDwgVp8scJaMyAiEjQ8L2JIHzA60NeRKS8hr8zEBGR3iWaDMxsqpm9YWZP55V9zMzuNrN/RL83TDKG/AFnGlgmIlJa0ncG1wO7FZSdAcxw99HAjGg7EbkBZwsWhIXOcgPLlBBERLpLNBm4+/3AkoLivYFp0eNpwD5Jnb/UgDMNLBMRKZZGm8FG7r4QIPr98XI7mtkEM5tpZjMXLVpU8YnKDSDTwDIRke4y3YDs7lPcfZy7jxs+fHjFx5cbQKaBZSIi3aWRDF43sxEA0e83kjpRqQFnGlgmIlIsjWRwJ3B49Phw4I6kTlQ44EwDy0RESkt0cRszuxHYGRgGvA6cC/wWuBkYBbwIHOjuhY3MRbS4jYhI5eIubpPoCGR3P6TMU7smeV4REalMphuQRUSkNpQMREREyUBERJQMRESEhHsTVZOZLQIWxNx9GPBmguHUgq4hG3QN2VDv15Bm/K3u3uuo3bpJBpUws5lxulJlma4hG3QN2VDv11BKjtAPAAAExklEQVQP8auaSERElAxERKRxk8GUtAOoAl1DNugasqHeryHz8Tdkm4GIiFSmUe8MRESkAkoGIiJS38nAzHYzs7lmNs/MitZSNrMjzGyRmc2Ofr6TRpzlmNlUM3vDzJ4u87yZ2RXR9T1pZtvXOsbexLiGnc3s7bz34Jxax9gbMxtpZvea2Rwze8bMvldin8y+FzHjz/T7YGbrmtnfzOyJ6BrOL7HPOmZ2U/QePGJmbbWPtLyY15DdzyR3r8sfoAV4HtgcWBt4AvhMwT5HAFelHWsP1/CvwPbA02We3x34A2DAjsAjacfch2vYGfhd2nH2cg0jgO2jx+sDz5X4t5TZ9yJm/Jl+H6K/60eixwOBR4AdC/Y5HrgmenwwcFPacffhGjL7mVTPdwY7APPc/QV3/wD4DbB3yjFVxN3vB3pay2Fv4Fce/BUYklslLitiXEPmuftCd38sevwuMAfYpGC3zL4XMePPtOjvuizaHBj9FPZu2RuYFj2+BdjVzKxGIfYq5jVkVj0ng02Al/K2X6b0f4D9o9v6W8xsZG1Cq5q415h1X4hunf9gZlulHUxPoqqHsYRvdfnq4r3oIX7I+PtgZi1mNpuwFO7d7l72PXD31cDbwNDaRtmzGNcAGf1MqudkUOobQWEWvgtoc/dtgXtY862iXsS5xqx7jDA3ynbAlYSV7jLJzD4C3Aqc4u7vFD5d4pBMvRe9xJ/598HdO919DLApsIOZbV2wS+bfgxjXkNnPpHpOBi8D+Vl1U+DV/B3cfbG7vx9t/hz4bI1iq5ZerzHr3P2d3K2zu08HBprZsJTDKmJmAwkfpB3ufluJXTL9XvQWf728DwDuvhS4D9it4KkP3wMzGwBsQEarKMtdQ5Y/k+o5GTwKjDazzcxsbUKD0p35OxTU6e5FqEutJ3cC3456suwIvO3uC9MOqhJm9i+5el0z24Hwb25xulF1F8V3HTDH3S8ts1tm34s48Wf9fTCz4WY2JHo8CBgPPFuw253A4dHjA4D/9ahVNgviXEOWP5MSXQM5Se6+2sxOBP5I6Fk01d2fMbMLgJnufidwspntBawmfIM4IrWASzCzGwm9PIaZ2cvAuYRGJ9z9GmA6oRfLPGA5cGQ6kZYX4xoOAI4zs9XACuDgLP0HjuwEHAY8FdX3ApwFjIK6eC/ixJ/192EEMM3MWgiJ6mZ3/13B/+frgP82s3mE/88HpxduSXGuIbOfSZqOQkRE6rqaSEREqkTJQERElAxERETJQEREUDIQERGUDER6ZGadeTNMzjaztrwZQB83s2fN7Cd5+59nZqcVvMb8rA7wEsmp23EGIjWyIppe4EPR/D8PuPse0eCix83sdnd/KI0ARapBdwYi/eDuK4DZZHDSOpFK6M5ApGeD8kb1/tPd981/0sw2BEYD99c8MpEqUjIQ6VlRNVHkS2b2JPAp4CJ3fy0qLzekX0P9JdNUTSTSNw9E0xBvQ5jzJ5cwFgMbFuy7PrC0lsGJVErJQKQf3P054ELg9KjofmAvM1sfwMz2A55w986UQhSJRdVEIv13DXCamW3m7k+a2VXAg2bmhBWvsrPouUgZmrVURERUTSQiIkoGIiKCkoGIiKBkICIiKBmIiAhKBiIigpKBiIgA/weS7ufBvRG8cQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def viz_polynomial():\n",
    "    plt.scatter(y,X,color='blue')\n",
    "    plt.plot(y,lin2.predict(poly.fit_transform(y)),color='red')\n",
    "    plt.title('FRU vs FRU score')\n",
    "    plt.xlabel(\"FRU\")\n",
    "    plt.ylabel(\"FRU_score\")\n",
    "    plt.show()\n",
    "    return\n",
    "viz_polynomial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.99582301])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=lin2.predict(poly.fit_transform([[2\n",
    "\n",
    "]]))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score,mean_squared_error,r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rmse_train=np.sqrt(mean_squared_error(y_train,y_train_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r2_train=r2_score(y_train,y_train_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Root meanSquared error is 1.662187747651971\n"
     ]
    }
   ],
   "source": [
    "print(\"The Root meanSquared error is\",format(rmse_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8966721424194617"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[21. 12. 12. 18. 18.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-71c408e87d44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_poly_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpoly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1438\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1439\u001b[0m         \"\"\"\n\u001b[1;32m-> 1440\u001b[1;33m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1441\u001b[0m         combinations = self._combinations(n_features, self.degree,\n\u001b[0;32m   1442\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteraction_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    550\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[21. 12. 12. 18. 18.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "X_poly_test=poly.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_predicted=lin2.predict(X_poly_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_test=np.sqrt(mean_squared_error(y_test,y_test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Root meanSquared error is 2.8424269516906335\n"
     ]
    }
   ],
   "source": [
    "print(\"The Root meanSquared error is\",format(rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train=r2_score(y_test,y_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9978830032764311"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
